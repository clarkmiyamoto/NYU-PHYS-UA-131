%%% Document Formatting
\documentclass[12pt,fleqn]{article}
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=0.75in,
            right=0.75in,
            top=0.8in,
            bottom=0.8in,
            footskip=.25in]{geometry}
\setlength\parindent{10pt} % No indent

%%% Imports
% Mathematics
\usepackage{amsmath} % Math formatting
\numberwithin{equation}{section} % Number equation per section
\DeclareMathOperator{\Tr}{Tr}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{example}{Example}

\usepackage{amsmath}
\usepackage{amsfonts} % Math fonts
\usepackage{amssymb} % Math symbols
\usepackage{mathtools} % Math etc.
\usepackage{slashed} % Dirac slash notation
\usepackage{cancel} % Cancels to zero
\usepackage{empheq}
\usepackage{breqn}
\usepackage{mathrsfs}


% Visualization
\usepackage{graphicx} % for including images
\graphicspath{ {} } % Path to graphics folder
\usepackage{tikz}
% Needed for \NewDocumentEnvironment and \IfNoValueTF
\usepackage{xparse}

% Needed for the pgfplots-based delta drawings I shared
\usepackage{pgfplots}
\pgfplotsset{compat=1.15} % or any recent version you have

% (Optional) nicer arrowheads for TikZ/pgfplots
\usetikzlibrary{arrows.meta}

\usepackage{caption} % for \captionof



%%% Formating
\usepackage{hyperref} % Hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\urlstyle{same}

\usepackage{mdframed} % Framed Enviroments
\newmdenv[ 
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep
]{sidework} % Side-work Environment

\newcounter{problem}
\NewDocumentEnvironment{problem}{o}{%
  \refstepcounter{problem}%
  \IfNoValueTF{#1}
    {\def\problem@title{Problem~\theproblem}}
    {\def\problem@title{Problem~\theproblem ~(#1)}}%
  \begin{mdframed}[
    linecolor=black,
    linewidth=0.5pt,
    backgroundcolor=blue!2.5,
    innertopmargin=0pt,
    innerbottommargin=10pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    frametitlefont=\bfseries,
    frametitle=\problem@title,
  ]
}{%
  \end{mdframed}
} % Practice problem environment

\newcounter{answer}
\NewDocumentEnvironment{answer}{o}{%
  \refstepcounter{answer}%
  \IfNoValueTF{#1}
    {\def\answer@title{Answer~\theanswer}}
    {\def\answer@title{Answer~\theanswer ~(#1)}}%
  \begin{mdframed}[
    linecolor=black,
    linewidth=0.5pt,
    backgroundcolor=green!2.5,
    innertopmargin=0pt,
    innerbottommargin=10pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    frametitlefont=\bfseries,
    frametitle=\answer@title,
  ]
}{%
  \end{mdframed}
}

% Griffiths cursive `r`
\def\rcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{assets/script_r/ScriptR.pdf}}$}}}
\def\brcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{assets/script_r/BoldR.pdf}}$}}}
\def\hrcurs{{\mbox{$\hat \brcurs$}}}



\usepackage{lipsum} % Lorem Ipsum example text




%%%%% ------------------ %%%%%
%%% Title
\title{E\&M I Recitation Notes}
\author{Clark Miyamoto (cm6627@nyu.edu)}
\date{Fall 2025}
\begin{document}

\maketitle



\tableofcontents

\section{Note to Reader}
Problems more relevant to this class have an asterisk $(*)$, the other problems are good for your progression as a physicist. I added a lot of problems because I feel more comfortable after I do the same problem a couple times, but most people don't need that much practice-- feel free to a problem or two and move on.

\newpage
\section{Week 1: Delta Functions}
You know how in Physics 101, everything was a point particle? Well in E\&M we want to build a theory which can talk about point particles (like a single electron), but also a distribution of particles (like a slab of charged metal). You can imagine if we define the point particle in a naive way, the mathematical computation will turn out wrong... This is where we introduce the \textbf{delta function}.
\begin{sidework}
	This will be a computational introduction, i.e. how we use and manipulate these functions. If you're more of a proper mathematician, I recommend looking at my friend \href{https://notes.panos.wiki/Analysis+Distributions}{Panos's notes (https://notes.panos.wiki)}. He has a very well written notes for mathematicians working on physics.
\end{sidework}
\subsection{Kronecker Delta (Discrete)}
As always, we start discrete and then promote it to be continuous.
\begin{definition}
	[Kronecker Delta]
	\begin{align}
		\delta_{ij} = \begin{cases}
			1 & \text{if } i = j\\
			0 & \text{otherwise}
		\end{cases}
	\end{align}
	where $i,j$ are taken to be integers. It is usually unit-less.
\end{definition}
While this notation looks scary, if we interpret the indices as the row/column of a matrix, we realize this is just the identity matrix.
\begin{align}
	\mathbb I & = \begin{pmatrix}
		1 & 0 & 0 & \\
		0 & 1 & 0 & ...\\
		0 & 0 & 1 & \\
	  	  & \vdots & & \ddots
	\end{pmatrix} \implies [\mathbb I]_{ij}  = \delta_{ij}
\end{align}
In physics, this scary feeling will come up a lot. You'll be faced with a weird expression with kronecker deltas, etc. but you'll just realize that it ends up being a regular-old matrix operation. 
\begin{problem}[Properties of Kronecker] There are a couple fundamental properties of Kronecker deltas. They're pretty easy, so it's your job to show them!
	\begin{enumerate}
		\item Symmetry: $\delta_{ij} = \delta_{ji}$
		\item Contraction: $\sum_j a_j \delta_{ij} = a_i$
		\item Dimension: $\sum_{i=1}^n \delta_{ii} = n$
	\end{enumerate}
\end{problem}
Now we can apply our properties to break-down an expression with Kronecker deltas.
\begin{sidework}
	\emph{Example:} Consider the expression $\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j$. If we interpret $a_i$ ($b_i$) as being the $i$'th entry from vector $\vec a$ ($\vec b$). What fundamental vector operation is this expression?
\begin{align}
	\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j & = \sum_{i=1}^n a_i b_i & \text{Contraction property} \\
	& = \vec a \cdot \vec b & \text{By definition of dot product}
\end{align}
Wow it's just a dot product. \\
\\
Another way to see this is to use the identity matrix fact.
\begin{align}
	\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j = \vec a^T ~\mathbb I ~\vec b = \vec a \cdot \vec b
\end{align}
Going back to the first proof. We that Kronecker deltas allow us to evaluate sums, this is because it is zero when $i \neq j$, so the only remaining component is $i= j$.
\end{sidework}
To get you comfortable with this, here's a couple of practice problems.
\begin{problem}[Kroneckers are not scary!]
	Let $A$ be a matrix, where $A_{ij} \equiv [A]_{ij}$ are the $i,j$'th entries of the matrix. What are the following expressions in-terms of simple linear algebra operations.
	\begin{enumerate}
		\item $\sum_{ij} \delta_{ij} A_{ij}$ 
		\item $\sum_{k\ell} \delta_{jk} \delta_{i \ell} A_{\ell k}$
		\item $\sum_{k\ell} \frac{1}{2} (\delta_{ij} \delta_{j\ell} + \delta_{i \ell} \delta_{jk}) A_{k\ell} $
	\end{enumerate}
\end{problem}


\subsection{Dirac Delta (Continuous)}
\subsubsection{Definition and Properties}
\begin{definition}
	[Dirac Delta/Delta Function, Heuristic] The delta function $\delta(x)$ is defined by the properties
	\begin{align}
		\delta(x) & = \begin{cases}
			\infty & \text{if } x = 0\\
			0 & \text{otherwise}
		\end{cases} ~~~~~~~\text{s.t. } \int_{\mathbb R} \delta(x)d x  = 1
	\end{align}
\end{definition}
\begin{center}
\begin{tikzpicture}[scale=0.8]
  \begin{axis}[
    xlabel={$x$},
    ymin=0, ymax=2,
    xmin=-3, xmax=3,
    xtick={-2,-1,0,1,2},
    ytick=\empty,
    clip=false
  ]
    \addplot[domain=-3:3, samples=2] {0};
    \draw[->, thick] (axis cs:0,0) -- (axis cs:0,1.8)
      node[left] {$\delta(x)$};
  \end{axis}
\end{tikzpicture}

\captionof{figure}{Graphical representation of the Dirac delta function 
$\delta(x)$ at the origin. The arrow indicates an “infinite” spike 
at $x=0$, with total integral equal to 1.}
\end{center}

What is particularly funky, is this is not a function! It is a \textbf{distribution}! For our purposes, this means \underline{delta functions are only defined when being integrated against "well-behaved" functions}. So if you ever see an equality between a delta functions, these are only defined when you integrate both sides of the equation.\\
\\
Let me say this again a little more formally
\begin{definition}[Dirac Delta / Delta Function, Less Heuristic] Let $\varphi$ be a arbitrary test function (an infinitely differentiable function with compact support). For our purposes, this means $\varphi(x)$ has the following properties
\begin{itemize}
	\item For all $n \in \mathbb N$, the derivative is finite $\frac{d^n}{dx^n}\varphi(x) < \infty$ 
	\item It goes to zero "quick-enough" as $x \to \infty$; implying it is integrable $\int_{-\infty}^\infty \varphi(x) dx < \infty$.
	\begin{sidework}
		In physics, quick-enough depends on which sub-discipline you're studying. In E\&M we often mean the \href{https://en.wikipedia.org/wiki/Sommerfeld_radiation_condition}{Sommer Field radiation condition}. 
	\end{sidework}
\end{itemize}
The delta function is defined as
\begin{align}
	\int \varphi(x) \delta(x) dx = \varphi(0)
\end{align}
for all $\varphi$ test function.
\end{definition}
\textbf{Properties:}
\begin{itemize}
	\item Units: $\delta(x)$ has units $[x]^{-1}$.
	\begin{sidework}
		\emph{Proof:} We can see this by inspecting the integral
		\begin{align}
			\int \delta(x) dx = 1
		\end{align}
		The measure $dx$ has units $[x]$, and the RHS has no units. Therefore $\delta(x)$ must have units $[x]^{-1}$. QED.
	\end{sidework}
	\item Scaling
		\begin{align}
			\delta(a x) = \frac{\delta(x)}{|a|}
		\end{align}
		\begin{sidework}
	Remember, when you see $"="$ equalities, this really mean 
	"these are equal when being integrated against an arbitrary test function $\varphi$".\\
	\\
	So when I write
	\begin{align}
		\delta(a x) \text{ "=" } \frac{\delta(x)}{|a|}
	\end{align}
	What I implicitly mean is
	\begin{align}
		\int \delta(a x) \varphi(x) dx = \int \frac{\delta(x)}{|a|} \varphi(x) dx, ~~\forall \varphi \in \text{Test Functions}
	\end{align} So now let's prove it!\\
	\\
	\emph{Proof:} Consider the LHS being integrated against an arbitrary test function. We'll find it becomes the RHS We'll have to do this twice, once with $a = |a| > 0$ and $a = -|a| < 0$. Let's do the positive case:
	\begin{align}
		\int_{-\infty}^\infty \delta (ax) \varphi (x) dx & = \int_{-\infty}^\infty \frac{\delta(x)}{a} \varphi(x/a) dx  & u\text{-sub}~ x \mapsto x/a\\
		& = \frac{1}{a} \varphi(0) & \text{By def: }\int \delta(x) g(x) dx = g(0)
	\end{align}
	So it effectively acts like $\delta(x)/a$. If you instead $u$-sub'ed $x\mapsto - x /|a|$, you'll have gotten $\varphi(0) / a$. Thus we've shown
	\begin{align}
		\int \delta(ax) \varphi(x) dx = \int \frac{\delta(x)}{|a|} \varphi(x) dx
	\end{align} QED.
\end{sidework}
	\item Symmetry: by consequence of scaling
		\begin{align}
			\delta(-x) = \delta(x)
		\end{align}
	\item Translation
		\begin{align}
			\int \varphi(x) \delta(x- x') = \varphi(x')
		\end{align} 
	\item Variational derivative:
	\begin{align}
		\frac{\delta}{\delta \varphi(x)} \varphi(y) = \delta(x-y)
	\end{align}
\end{itemize}



\begin{problem}[$\star$ Charge Distributions]
	The total charge $Q$ is related to the charge distribution $\rho(\mathbf r)$ by $Q = \int \rho(\mathbf r) d^3 r$. Write 
$\rho(\mathbf r)$ in terms of $\delta$ functions for the following systems:
	\begin{itemize}
		\item Point charge w/ total charge $Q$ centered at the origin. Do this in cartesian coordinates $(x,y,z)$, cylindrical coordinates $(\rho, \phi, z)$, and finally in spherical coordinates $(r, \phi, \theta)$.
		\item Infinite cylinder with radius $R$, with surface charge $\sigma$.
		\item Spherical shell of radius $R$, with total charge $Q$. 
	\end{itemize}
	\emph{Discussion:} How would you check your answer is correct? Recall equalities of expressions with delta functions are only defined in-terms of integrals, what is the meaning of what you've wrote down?
\end{problem}

\subsubsection{Showing something is a delta function in disguise}
In lecture, Grier mentioned the divergence of the vector field $\mathbf F(\mathbf r) = \frac{\mathbf{\hat{r}}}{\mathbf r^2}$ was
\begin{align}
	\boxed{\nabla \cdot \left ( \frac{\mathbf{\hat{r}}}{\mathbf r^2}\right) = 4\pi ~\delta^{(3)} (\mathbf r - \mathbf r')}
\end{align}
where $\nabla$ is the gradient operator.\\
\\
If you naively plug n' chug, you'd find $\nabla \cdot \left ( \frac{\mathbf{\hat{r}}}{\mathbf r^2}\right) =^? 0$. This is an artifact of a coordinate singularity (like how can you differentiate at $r \to 0$, it's not continuous). \\
\\
Grier makes a physics argument using $\nabla \cdot \mathbf E =  \rho / \epsilon_0$ as to why these are related. Here I'll show a mathematical proof.
\\
\\
\emph{Proof:} Recall a delta function is defined inside an integral, so we'll integrate this against an arbitrary scalar test function $\varphi : \mathbb R^3 \to \mathbb R$, and show it behaves like a delta function.\\
\\
First we can integrate by parts by noting $\text{div}(\vec f g) = (\nabla \cdot \vec f) g + \vec f \nabla g$
\begin{align}
	\int \varphi(\mathbf r) \nabla \cdot \left( \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3 r & = \underbrace{\int \nabla\cdot (\varphi(\mathbf r) \frac{\mathbf{\hat{r}}}{\mathbf r^2}) d^3r}_{\text{Term (1)}} - \underbrace{\int \nabla\varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} d^3r}_{\text{Term (2)}}
\end{align}


Term (1): Use the divergence theorem
\begin{align}
	\int_V \nabla\cdot \left(\varphi(\mathbf r) \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3r & = \int_{\partial V} \frac{\varphi(\mathbf r)}{\mathbf r^2}  \mathbf{\hat r} \cdot d\mathbf S \\
	& = 0 & \varphi \text{ vanishes at infinity}
\end{align}

Term (2): Use spherical coordinates
\begin{align}
	\int \nabla\varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} d^3r & = \int \nabla \varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} ~r^2 dr   d \Omega \\
	&= 4\pi   \int_0^\infty \frac{\partial \varphi}{\partial r}  dr \\
	& = 4\pi \varphi(r) \Big|_{r=0}^\infty & \text{Fundamental Theorem of Calculus}\\
	&  = 4\pi ( \cancelto{0}{\varphi(\infty)} - \varphi(0)) & \varphi \text{ vanishes at infinity}\\
	& = -4\pi \varphi(0)
\end{align}
Putting this all together
\begin{align}
	\int \varphi(\mathbf r) \nabla \cdot \left( \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3 r = 4\pi \varphi(0)
\end{align}
Remember, if it acts like a delta function, it is a delta function by definition. QED.\\
\\
In general, proofs of this nature have lots of by parts calculations.
\begin{problem}[Representations of Delta Functions] The delta function emits an infinite number of representations. These are some common ones. Prove that they're actually delta functions!
	\begin{itemize}
		\item Laplacian Representation:
		\begin{align}
			\delta^{(3)} (\mathbf r - \mathbf r') = - \frac{1}{4\pi} \nabla^2 \frac{1}{|\mathbf r - \mathbf r'|}
		\end{align}
		where $|\cdot |$ is the magnitude of a vector.
		\item $(\star)$ Fourier Representation (very important for quantum mechanics!)
		\begin{align}
			\delta(x) = \int_{\mathbb R} \frac{d k}{2\pi} ~ e^{i k x}
		\end{align}
		
		\item Gaussian Representation
		\begin{align}
			\delta(x) = \lim_{\epsilon \to 0^+}  \frac{1}{\epsilon \sqrt{2\pi}} \exp \left(-\frac{1}{2} \frac{x^2}{\epsilon^2} \right)
		\end{align}
	\end{itemize}
	A good question to ask yourself, how do these generalize when $x \in \mathbb R^d$?
\end{problem}

\newpage
\section{Week 2: Gauss' Law}
This week we get some practice using Gauss' Law
\begin{problem}[$\star$ Spherical Gauss' Law]
	Consider a sphere of radius $R$ with total charge $Q$. The charge is spread out uniformly across the entire sphere. A common question is, what is the electric field $\mathbf E(\mathbf r)$ associated with this system? \\
	\\
	Let's derive this together using Gauss' Law. I'll ask you guiding questions to get you half the way there, and you piece together the rest!
	\begin{enumerate}
		\item Draw and write down the charge distribution $\rho(\mathbf r)$ of this system.
		\item How do we relate the electric field $\mathbf E(\mathbf r)$, the charge distribution $\rho(\mathbf r)$, and integration? 
		\item You've probably hit a point where you need to specify a test volume $\mathcal V$ to integrate over. What volume should you take? Why? Draw it 
		\item What is the surface area $S$ and volume $V$ of your chosen test volume $\mathcal V$?  
		\item Your integral equation in part (2) requires associated measures for said test surface $\mathcal S = \partial \mathcal V$ \& volume $\mathcal V$. So what are the corresponding measures of the surface integral $d S$ and volume integral $dV$? What vector do you associated to the measure of the surface integral $d \mathbf S$?
		\item What is the electric field $\mathbf E(\mathbf r)$ inside and outside the sphere. Show you recover a point charge outside.
	\end{enumerate}
	Note: on a test, you'll be asked to determine the electric field $\mathbf E(\mathbf r)$ without being told to use $\mathbf E(\mathbf r)$, and without any of the guiding in-between steps. Think about how you could have solve the problem end-to-end?
	\\
	\\
	Discussion:
	\begin{itemize}
		\item In part 2, why did I guide you towards integration? Why is the differential form of Gauss' Law not useful here?
		\item For this problem, which method did you prefer? Integrating the charge distribution $\mathbf E(\mathbf r) = \frac{1}{4\pi \epsilon_0} \int \frac{\rho(\mathbf r')}{\rcurs^2} \hrcurs d^3 r'$	or using the integral form of Gauss' Law $\int_{\partial V} \mathbf E(\mathbf r) \cdot d \mathbf S = \int_V \rho(\mathbf r) d^3 r $? Can you given an example system where you'd prefer using the other method?
		\item Notice you had symmetries in your charge distribution $\rho(\mathbf r)$. How does this information constrain the possible electric fields $\mathbf E(\mathbf r)$ that can result?
	\end{itemize}
\end{problem}
\begin{answer}
	Note: this isn't the answer to everything, but me just working through the problem for future reference. Not very pedagogical.\\
	\\
	To find the resulting electric field $\mathbf E(\mathbf r)$ (in spherical coordinates $\mathbf r = (r, \phi, \theta)$, first stat by writing down Gauss's Law in integral form
	\begin{align}
		\int_{\partial V} \mathbf E(\mathbf r) \cdot d \mathbf S = \frac{1}{\epsilon_0}\int \rho (\mathbf r) d^3 r
	\end{align}
	You always want the symmetry of your test volume $V$ to match the symmetry of the charge distribution. So you should choose $V$ to be a sphere of radius $r$.\\
	\\
	The charge distribution changes depending on whether you're inside or outside the sphere, so we should expect a change in electric field strength.\\
	\\
	Case: Inside sphere $(r \leq R)$.
	\begin{align}
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{1}{\epsilon_0} \frac{Q}{\frac{4}{3}\pi R^3} \int_{\Omega} \int_0^r  r^2 d\Omega\\
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{1}{4\pi \epsilon_0}\frac{3Q}{R^3} \left(\frac{r^3}{3}\right) \left(4\pi\right)\\
		\mathbf E(\mathbf r) \cdot \mathbf{\hat r} (r^2) (4\pi) & =  \frac{1}{4\pi \epsilon_0}\frac{3Q}{R^3} \left(\frac{r^3}{3}\right) \left(4\pi\right)\\
		\mathbf E(\mathbf r) & = \frac{1}{4\pi \epsilon_0} \frac{Q r}{R^3} \mathbf{\hat r}
	\end{align}
	where $d\Omega$ integrates over the solid-angle.\\
	\\
	Case: Outside sphere $(r > R)$
	\begin{align}
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{Q_{encl}}{\epsilon_0} = \frac{Q}{\epsilon_0}\\
		\mathbf E(\mathbf r) & = \frac{1}{4\pi \epsilon_0}\frac{Q}{r^2} \mathbf{\hat r}
	\end{align}
	So your final answer is
	\begin{align}
		\boxed{\mathbf E(\mathbf r) = \begin{cases}
			\frac{1}{4\pi \epsilon_0} \frac{Q r}{R^3} \mathbf{\hat r} & r \leq R\\
			\frac{1}{4\pi \epsilon_0}\frac{Q}{r^2} \mathbf{\hat r} & r > R
		\end{cases}}
	\end{align}
	
\end{answer}

\begin{problem}[$\star$ Flux Conundrums]
	Here we explore a *slight* paradoxes which arise when we use Gauss' Law. 
	\begin{enumerate}
		\item Consider a uniformly charged sphere of radius $R$ with total charge $Q$. Use a test volume $\mathcal V$ which is a sphere of radius $r$. What is the electric flux inside and outside the sphere? What is the electric field (see previous problem)?
		\item Now add a point charge outside of the sphere at a distance $L$ away (that is $L > R$). Now using the same test volume $\mathcal V$, what is the electric flux inside and outside the sphere? What is the electric field now?
	\end{enumerate}
	What changed, what stayed the same. Why are you still a happy physicist?
\end{problem}



\newpage
\section{Week 3: Multivariable Calculus}
Ok Grier feels like the students had a couple questions about multivariable calculus, so I think it'd be good to take a recitation to do a basic review of some concepts

\subsection{Multivariable Integration by parts}
Integration by parts in multiple variables seems like a daunting. I mean there's so many new derivative operators, so how can you memorize them all? Well the secret is not to memorize, but to figure out how derive! \\
\\
Let's start w/ 1D integration by parts, and see how it generalizes to multiple dimensions.

\subsubsection{Integration by parts in 1D}
Recall the famous: integration by parts
\begin{align}
	\int_I u dv = u v \Big |_{\partial I} - \int_I v du
\end{align}
where $I$ is some interval on the real line (for example $I = [0, +\infty)$).\\
\\
\emph{Proof:}
Consider the total derivative of a function $f(x_1, ..., x_n)$
\begin{align}
	df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i
\end{align}
This has a product rule, meaning for functions $u$ and $v$:
\begin{align}
	d(uv) = (du) \cdot  v + u \cdot (dv)
\end{align}
\begin{problem}
	Prove the product rule of the total derivative.
\end{problem}
\begin{sidework}
	A small proof the product rule of the total derivative 
	\begin{align}
		d(fg) & =  \sum_i \frac{\partial(fg)}{\partial x_i}  dx_i \\
		& = \sum_i \left(\frac{\partial f}{\partial x_i} g + f \frac{\partial g}{\partial x_i}\right)dx_i \\
		& = \left(\sum_i \frac{\partial f}{\partial x_i} dx_i  \right)\cdot  g + f \cdot \left(\sum_i \frac{\partial g}{\partial x_i} dx_i  \right)\\
		& = (df)~g + f~ (dg)
	\end{align}
\end{sidework}
Notice the LHS of the integration by parts formula is $u (dv)$, so we can use our product rule from the total derivative to get
\begin{align}
	u dv = d(uv) - v du \implies \int u ~dv = \int d(uv) - \int v ~du
\end{align}
By the fundamental theorem of calculus $\int_I df = f \Big|_{\partial I}$. So you're left with
\begin{align}
	\int u~ dv = uv \Big|_{\partial I} - \int v ~du
\end{align}
QED.
\subsubsection{Integration in Multiple Variables}
The natural generalization of the derivative is the gradient $\nabla$. Now let's do the same derivation, and just turn the crank. \\
\\
For example, consider
\begin{align}
	\int_V f (\nabla g) ~ dV = \int_{\partial V} fg \mathbf ~d\mathbf S - \int_V g \nabla f ~ dV
\end{align}
\emph{Proof:} Notice the product rule
\begin{align}
	\nabla(fg) = (\nabla f) g + f (\nabla g) \implies f(\nabla g) = \nabla(fg) - (\nabla f) g
\end{align}
Do a volume integration over both sides
\begin{align}
	\int_V f(\nabla g) dV = \int_V \nabla (fg) dV - \int_V (\nabla f) g dV
\end{align}
To simplify the $\nabla(fg)$ term, we can use the divergence theorem. Consider the vector field $\mathbf F_i = fg ~ \mathbf{\hat e_i}$, where $\mathbf{\hat e_i}$ is the unit vector in the $i$'th direction. This means $\nabla(fg) = \sum_i \frac{\partial}{\partial x_i} \mathbf F_i = \sum_i (\nabla \cdot \mathbf F_i) \mathbf{\hat e_i}$.\begin{align}
	\int_V \nabla (fg) dV & = \sum_i \mathbf{\hat e_i} \int_V (\nabla \cdot \mathbf F_i)  dV \\
	& =  \sum_i \mathbf{\hat e_i}\int_{\partial V} fg ~ \mathbf{\hat e}_i\cdot d \mathbf S\\
	&  = \int_{\partial V} fg~ d\mathbf S
\end{align}
Putting it togther we find
\begin{align}
	\int_V f(\nabla g) dV = \int_{\partial V} fg ~d\mathbf S - \int_V g\nabla f~ dV
\end{align}
QED.
\begin{problem}
	Now it's your turn, prove the following multivariable integration by parts formulas
	\begin{align}
		\int_V \nabla f \cdot \nabla g ~ dV & = \int_{\partial V} f\nabla g \cdot d \mathbf S - \int_V f\nabla^2 g ~ dV\\
		\int_V (f \nabla^2 g - g \nabla^2 f) dV& = \int_{\partial V} (f \nabla g - g \nabla f) \cdot d \mathbf S
	\end{align}
	Think about what product rule do you need to get yourself there?
\end{problem}
\newpage
\section{Week (?) Helmholtz Theorem}

\newpage
\section{Week ?: Fourier Series}
If you're a STEM major, you have probably seen \href{https://www.youtube.com/watch?v=spUNpyF58BY}
{3Blue1Brown's video on Fourier Analysis} (if you haven't, you should watch it now!). Basically he visualizes how complex signals can be decomposed into a linear combinations of pure signals (sines and cosines). So if you haven't, go watch it now.

Now that you're back, I'll try to introduce it again, but lay the notation \& computational workflow for using this to solve some problems.

\subsection{Inner Products and Orthogonality}
Recall in linear algebra, we had these things called \textbf{inner products}. 
\begin{definition}
	[Inner Product] Let $V$ be a vector space. Consider the operation $\langle \cdot, \cdot \rangle : V \times V \to \mathbb R$. It is an \textbf{inner product} when it has the following properties:
	\\
	\\
	Let $a,b,c \in V$, and $\alpha \in \mathbb R$
	\begin{itemize}
		\item Bilineraity (linearity in both arguments): 
		\begin{align}
			\langle \alpha (a + b) , c\rangle & = \alpha \langle a, c\rangle  + \alpha \langle b , c\rangle\\
			\langle a, \alpha (b + c)\rangle &  = \alpha \langle a, b\rangle + \alpha \langle a, c\rangle
		\end{align}  
		\item Symmetry: $\langle a, b\rangle = \langle b, a\rangle$
		\item Positive-Definite: $\langle a, a\rangle \geq 0$, and equal if and only if $a=0$.
	\end{itemize}
\end{definition}
Ok cool definition, but this intuitively allowed us to determine how "aligned" two vectors were with each other. So for example, in $\mathbb R^3$, we had
\begin{align}
	\langle \vec a, \vec b\rangle = \vec a^T \vec b =  \sum_i a_i b_i =|a||b| \cos \theta
\end{align}
If $\langle \vec a , \vec b\rangle = 0$, then $\theta = 90 deg$ so they were \textbf{orthogonal} (perpendicular) to each other.\\
\\
What if we wanted to talk about more exotic objects, such as the inner product between functions? Well, imagine you index your function like a vector $f(x_i) = f_i$, then the dot product would look like $\sum_i f_i g_i = \sum_i f(x_i ) g(x_i)$, add a measure $\Delta x$, and you'd get
\begin{align}
	\langle f, g\rangle = \sum_i f(x_i) g(x_i) \Delta x \simeq \int f(x) g(x) dx
\end{align}
Now we can detect for "orthogonality" between two functions. 
\begin{problem}[$\star$ Orthogonality of Cosines and Sines]
	Show the following is a valid inner product:
	\begin{align}
		\langle f, g \rangle = \frac{1}{L} \int_{-L}^L f(x) g(x) dx
	\end{align}
	Show cosines and sines of integer frequency ($n,m \in \{1, 2, 3, ...\}$) are orthogonal:
	\begin{align}
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \sin\left(\frac{m \pi x}{L} \right) \right\rangle & = 0 \\
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \cos\left(\frac{m \pi x}{L} \right) \right\rangle & =   \delta_{nm} \\
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \cos\left(\frac{m \pi x}{L} \right) \right\rangle  & =  \delta_{nm}
	\end{align}
	Online, you'll probably find different factors, just note the how the integral changes / the argument of the cosine \& sine.
\end{problem}

\subsection{Basis}
Recall in linear algebra that you had a basis to represent vectors. For example consider vectors $\mathbf v \in \mathbb R^3$, you can decompose it into a linear combination of basis vectors
\begin{align}
	\mathbf v = \begin{pmatrix}
		v_1 \\ v_2 \\ v_3 
	\end{pmatrix} = v_1 \begin{pmatrix}
		1 \\ 0 \\ 0
	\end{pmatrix} + v_2 \begin{pmatrix}
		0 \\ 1\\ 0
 	\end{pmatrix} + v_3 \begin{pmatrix}
 		0 \\ 0 \\ 1
 	\end{pmatrix}
\end{align}I'll denote the basis vectors as $\{\mathbf{\hat e}^{(i)}\}_{i=1}^3$. Lucky for us, I've chosen a set of orthogonal basis vectors, that is under an inner product $\langle \mathbf{\hat e}^{(i)}, \mathbf{\hat e}^{(j)} \rangle = \delta_{ij}$.


Sooo, what if your vector is in a really huge vector space $\mathbf v \in \mathbb R^d$? Well, same thing goes
\begin{align}
	\mathbf v = v_1 \begin{pmatrix}
		1 \\ 0 \\ 0 \\ \vdots
	\end{pmatrix} + v_2 \begin{pmatrix}
		0 \\ 1 \\ 0 \\ \vdots
	\end{pmatrix} + ... +  v_d \begin{pmatrix}
		\vdots \\ 0 \\ 0 \\ 1
	\end{pmatrix} = \sum_{i=1}^d v_i \mathbf{\hat e}^{(i)}
\end{align}
Ok, now keep this in your mind. Let's talk about functions, and I'll jump back to what I just talked about.\\
\\
So do you remember how you can Taylor series expand a function?
\begin{align}
	f(x) = \sum_{n=0}^\infty \underbrace{\frac{f^{(n)}(0)}{n!}}_{v_n} \underbrace{x^n}_{\mathbf{\hat e}^{(n)}}
\end{align}
Notice, this should remind you of the previous term. Heuristically, this look like your basis vectors are
\begin{align}
	f(x) = f(0) \begin{pmatrix}
		1 \\ 0 \\ 0 \\ \vdots
	\end{pmatrix} + f'(0) \begin{pmatrix}
		0 \\ x \\ 0 \\ \vdots
 	\end{pmatrix} + \frac{f''(0)}{2!} \begin{pmatrix}
 		0 \\ 0 \\ x^2 \\ \vdots
 	\end{pmatrix} + ...
\end{align}
Woah... So Clark what are you tryna say? Well, what if we could expand a function in an orthogonal basis? Whattt??

It takes a lot work, but basically the integer frequency cosines and sines form a complete basis. So now we can consider write a function a linear combination of the basis vectors (cosines and sines).
\begin{align}
	f(x) & = \sum_{n=0}^\infty A_n \cos\left(\frac{n \pi x}{L}\right) + \sum_{m=0}^\infty B_n \sin\left(\frac{n \pi x}{L}\right)\\
	& = A_0 + \sum_{n=1}^\infty A_n \cos\left(\frac{n \pi x}{L}\right) + \sum_{m=1}^\infty B_n \sin\left(\frac{n \pi x}{L}\right)
\end{align}

\subsection{Using Inner Products to Get Coefficients of Basis Vector}

Now you might ask: "Say I'm given an $f(x)$, how do I compute $A_0, A_n, B_n$?"
\\
\\
Let's do this in for discrete case, and then do it for functions. Consider the vector $v \in \mathbb R^d$ decomposed into its orthogonal basis
\begin{align} 
	\mathbf v = \sum_{i=1}^d v_i \mathbf{\hat e}^{(i)}
\end{align}
If I compute the inner product of this against an arbitrary basis vector, we find it picks out the associated coefficient
\begin{align}
	\mathbf{\hat e}^{(j) T}\mathbf v &= \sum_{i=1}^d v_i \mathbf{\hat e}^{(j) T} \mathbf{\hat e}^{(i)}\\
	\langle \mathbf{\hat e}^{(j)}, \mathbf v\rangle & = \sum_{i=1}^d v_i \langle \mathbf{\hat e}^{(j)},  \mathbf{\hat e}^{(i)} \rangle\\
	& =\sum_{i=1}^d v_i \delta_{ij}\\
	& = v_j
\end{align}
So let's use the same technique on derive the coefficients of the Fourier series. 
\begin{problem}[Coefficients of Fourier Series]
	Consider a function $f(x)$ represented in terms of its Fourier series
	\begin{align}
		f(x) = A_0 + \sum_{n=1}^\infty A_n \cos\left(\frac{n\pi x }{L}\right)  + \sum_{n=1}^\infty B_n \sin\left(\frac{n\pi x }{L}\right)
	\end{align}
	Derive a relationship between the coefficients and original function:
	\begin{align}
		A_0 & = \frac{1}{2L}\int_{-L}^L f(x) dx\\
		A_n & = \frac{1}{L}\int_{-L}^L f(x) \cos\left(\frac{n\pi x }{L}\right) dx\\
		B_n & = \frac{1}{L}\int_{-L}^L f(x) \sin\left(\frac{n\pi x }{L}\right) dx
	\end{align}
	Hint: Exploit the orthogonality that you've computed in the previous problem.
\end{problem}
Congrats! You've derived the Fourier series representation yourself! Be proud!


\newpage
\section{Appendix}

\subsection{Review of Vector Calculus}
\subsubsection{Coordinates}
In this section, and probably for everything else in these notes, we'll assume we work with functions that have domain on $\mathbb R^3$.\\
\\
Consider a vector $\vec r \in \mathbb R^3$. We can also decompose it a complete basis $\{\mathbf{\hat x}, \mathbf{\hat y}, \mathbf{\hat z}\}$, where the $\mathbf{\hat v}$ (hat symbol) signifies it is a unit vector.
\begin{align}
	\vec r  = x \mathbf{\hat x} + y \mathbf{\hat y} + z \mathbf {\hat z}
\end{align}




\subsubsection{Change of Coordinates of Differential Operators}
In classical mechanics, you probably had to work with differential operators. For example: the gradient $\vec \nabla$, the laplacian $\nabla^2 \equiv \vec \nabla \cdot \vec \nabla$, etc. However these have expressions depending on your coordinate system.
\begin{align}
	\vec \nabla & = \frac{\partial}{\partial x} \mathbf{\hat x} + \frac{\partial}{\partial y} \mathbf{\hat y} + \frac{\partial}{\partial z} \mathbf{\hat z}
\end{align}


 You might have noticed that certain problems are more amendable 














































\end{document}