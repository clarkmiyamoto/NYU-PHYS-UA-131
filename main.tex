%%% Document Formatting
\documentclass[12pt,fleqn]{article}
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=0.75in,
            right=0.75in,
            top=0.8in,
            bottom=0.8in,
            footskip=.25in]{geometry}
\setlength\parindent{10pt} % No indent

%%% Imports
% Mathematics
\usepackage{amsmath} % Math formatting
\numberwithin{equation}{section} % Number equation per section
\DeclareMathOperator{\Tr}{Tr}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{example}{Example}

\usepackage{amsmath}
\usepackage{amsfonts} % Math fonts
\usepackage{amssymb} % Math symbols
\usepackage{mathtools} % Math etc.
\usepackage{slashed} % Dirac slash notation
\usepackage{cancel} % Cancels to zero
\usepackage{empheq}
\usepackage{breqn}
\usepackage{mathrsfs}


% Visualization
\usepackage{graphicx} % for including images
\graphicspath{ {} } % Path to graphics folder
\usepackage{tikz}
% Needed for \NewDocumentEnvironment and \IfNoValueTF
\usepackage{xparse}

% Needed for the pgfplots-based delta drawings I shared
\usepackage{pgfplots}
\pgfplotsset{compat=1.15} % or any recent version you have

% (Optional) nicer arrowheads for TikZ/pgfplots
\usetikzlibrary{arrows.meta}

\usepackage{caption} % for \captionof



%%% Formating
\usepackage{hyperref} % Hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\urlstyle{same}

\usepackage{mdframed} % Framed Enviroments
\newmdenv[ 
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep
]{sidework} % Side-work Environment

\newcounter{problem}
\NewDocumentEnvironment{problem}{o}{%
  \refstepcounter{problem}%
  \IfNoValueTF{#1}
    {\def\problem@title{Problem~\theproblem}}
    {\def\problem@title{Problem~\theproblem ~(#1)}}%
  \begin{mdframed}[
    linecolor=black,
    linewidth=0.5pt,
    backgroundcolor=blue!2.5,
    innertopmargin=0pt,
    innerbottommargin=10pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    frametitlefont=\bfseries,
    frametitle=\problem@title,
  ]
}{%
  \end{mdframed}
} % Practice problem environment

\newcounter{answer}
\NewDocumentEnvironment{answer}{o}{%
  \refstepcounter{answer}%
  \IfNoValueTF{#1}
    {\def\answer@title{Answer~\theanswer}}
    {\def\answer@title{Answer~\theanswer ~(#1)}}%
  \begin{mdframed}[
    linecolor=black,
    linewidth=0.5pt,
    backgroundcolor=green!2.5,
    innertopmargin=0pt,
    innerbottommargin=10pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    frametitlefont=\bfseries,
    frametitle=\answer@title,
  ]
}{%
  \end{mdframed}
}

% Griffiths cursive `r`
\def\rcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{assets/script_r/ScriptR.pdf}}$}}}
\def\brcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{assets/script_r/BoldR.pdf}}$}}}
\def\hrcurs{{\mbox{$\hat \brcurs$}}}



\usepackage{lipsum} % Lorem Ipsum example text




%%%%% ------------------ %%%%%
%%% Title
\title{E\&M I Recitation Notes}
\author{Clark Miyamoto (cm6627@nyu.edu)}
\date{Fall 2025}
\begin{document}

\maketitle



\tableofcontents

\section{Note to Reader}
Problems more relevant to this class have an asterisk $(*)$, the other problems are good for your progression as a physicist. I added a lot of problems because I feel more comfortable after I do the same problem a couple times, but most people don't need that much practice-- feel free to a problem or two and move on.

\newpage
\section{Week 1: Delta Functions}
You know how in Physics 101, everything was a point particle? Well in E\&M we want to build a theory which can talk about point particles (like a single electron), but also a distribution of particles (like a slab of charged metal). You can imagine if we define the point particle in a naive way, the mathematical computation will turn out wrong... This is where we introduce the \textbf{delta function}.
\begin{sidework}
	This will be a computational introduction, i.e. how we use and manipulate these functions. If you're more of a proper mathematician, I recommend looking at my friend \href{https://notes.panos.wiki/Analysis+Distributions}{Panos's notes (https://notes.panos.wiki)}. He has a very well written notes for mathematicians working on physics.
\end{sidework}
\subsection{Kronecker Delta (Discrete)}
As always, we start discrete and then promote it to be continuous.
\begin{definition}
	[Kronecker Delta]
	\begin{align}
		\delta_{ij} = \begin{cases}
			1 & \text{if } i = j\\
			0 & \text{otherwise}
		\end{cases}
	\end{align}
	where $i,j$ are taken to be integers. It is usually unit-less.
\end{definition}
While this notation looks scary, if we interpret the indices as the row/column of a matrix, we realize this is just the identity matrix.
\begin{align}
	\mathbb I & = \begin{pmatrix}
		1 & 0 & 0 & \\
		0 & 1 & 0 & ...\\
		0 & 0 & 1 & \\
	  	  & \vdots & & \ddots
	\end{pmatrix} \implies [\mathbb I]_{ij}  = \delta_{ij}
\end{align}
In physics, this scary feeling will come up a lot. You'll be faced with a weird expression with kronecker deltas, etc. but you'll just realize that it ends up being a regular-old matrix operation. 
\begin{problem}[Properties of Kronecker] There are a couple fundamental properties of Kronecker deltas. They're pretty easy, so it's your job to show them!
	\begin{enumerate}
		\item Symmetry: $\delta_{ij} = \delta_{ji}$
		\item Contraction: $\sum_j a_j \delta_{ij} = a_i$
		\item Dimension: $\sum_{i=1}^n \delta_{ii} = n$
	\end{enumerate}
\end{problem}
Now we can apply our properties to break-down an expression with Kronecker deltas.
\begin{sidework}
	\emph{Example:} Consider the expression $\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j$. If we interpret $a_i$ ($b_i$) as being the $i$'th entry from vector $\vec a$ ($\vec b$). What fundamental vector operation is this expression?
\begin{align}
	\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j & = \sum_{i=1}^n a_i b_i & \text{Contraction property} \\
	& = \vec a \cdot \vec b & \text{By definition of dot product}
\end{align}
Wow it's just a dot product. \\
\\
Another way to see this is to use the identity matrix fact.
\begin{align}
	\sum_{i=1}^n \sum_{j=1}^n \delta_{ij} a_i b_j = \vec a^T ~\mathbb I ~\vec b = \vec a \cdot \vec b
\end{align}
Going back to the first proof. We that Kronecker deltas allow us to evaluate sums, this is because it is zero when $i \neq j$, so the only remaining component is $i= j$.
\end{sidework}
To get you comfortable with this, here's a couple of practice problems.
\begin{problem}[Kroneckers are not scary!]
	Let $A$ be a matrix, where $A_{ij} \equiv [A]_{ij}$ are the $i,j$'th entries of the matrix. What are the following expressions in-terms of simple linear algebra operations.
	\begin{enumerate}
		\item $\sum_{ij} \delta_{ij} A_{ij}$ 
		\item $\sum_{k\ell} \delta_{jk} \delta_{i \ell} A_{\ell k}$
		\item $\sum_{k\ell} \frac{1}{2} (\delta_{ij} \delta_{j\ell} + \delta_{i \ell} \delta_{jk}) A_{k\ell} $
	\end{enumerate}
\end{problem}


\subsection{Dirac Delta (Continuous)}
\subsubsection{Definition and Properties}
\begin{definition}
	[Dirac Delta/Delta Function, Heuristic] The delta function $\delta(x)$ is defined by the properties
	\begin{align}
		\delta(x) & = \begin{cases}
			\infty & \text{if } x = 0\\
			0 & \text{otherwise}
		\end{cases} ~~~~~~~\text{s.t. } \int_{\mathbb R} \delta(x)d x  = 1
	\end{align}
\end{definition}
\begin{center}
\begin{tikzpicture}[scale=0.8]
  \begin{axis}[
    xlabel={$x$},
    ymin=0, ymax=2,
    xmin=-3, xmax=3,
    xtick={-2,-1,0,1,2},
    ytick=\empty,
    clip=false
  ]
    \addplot[domain=-3:3, samples=2] {0};
    \draw[->, thick] (axis cs:0,0) -- (axis cs:0,1.8)
      node[left] {$\delta(x)$};
  \end{axis}
\end{tikzpicture}

\captionof{figure}{Graphical representation of the Dirac delta function 
$\delta(x)$ at the origin. The arrow indicates an “infinite” spike 
at $x=0$, with total integral equal to 1.}
\end{center}

What is particularly funky, is this is not a function! It is a \textbf{distribution}! For our purposes, this means \underline{delta functions are only defined when being integrated against "well-behaved" functions}. So if you ever see an equality between a delta functions, these are only defined when you integrate both sides of the equation.\\
\\
Let me say this again a little more formally
\begin{definition}[Dirac Delta / Delta Function, Less Heuristic] Let $\varphi$ be a arbitrary test function (an infinitely differentiable function with compact support). For our purposes, this means $\varphi(x)$ has the following properties
\begin{itemize}
	\item For all $n \in \mathbb N$, the derivative is finite $\frac{d^n}{dx^n}\varphi(x) < \infty$ 
	\item It goes to zero "quick-enough" as $x \to \infty$; implying it is integrable $\int_{-\infty}^\infty \varphi(x) dx < \infty$.
	\begin{sidework}
		In physics, quick-enough depends on which sub-discipline you're studying. In E\&M we often mean the \href{https://en.wikipedia.org/wiki/Sommerfeld_radiation_condition}{Sommer Field radiation condition}. 
	\end{sidework}
\end{itemize}
The delta function is defined as
\begin{align}
	\int \varphi(x) \delta(x) dx = \varphi(0)
\end{align}
for all $\varphi$ test function.
\end{definition}
\textbf{Properties:}
\begin{itemize}
	\item Units: $\delta(x)$ has units $[x]^{-1}$.
	\begin{sidework}
		\emph{Proof:} We can see this by inspecting the integral
		\begin{align}
			\int \delta(x) dx = 1
		\end{align}
		The measure $dx$ has units $[x]$, and the RHS has no units. Therefore $\delta(x)$ must have units $[x]^{-1}$. QED.
	\end{sidework}
	\item Scaling
		\begin{align}
			\delta(a x) = \frac{\delta(x)}{|a|}
		\end{align}
		\begin{sidework}
	Remember, when you see $"="$ equalities, this really mean 
	"these are equal when being integrated against an arbitrary test function $\varphi$".\\
	\\
	So when I write
	\begin{align}
		\delta(a x) \text{ "=" } \frac{\delta(x)}{|a|}
	\end{align}
	What I implicitly mean is
	\begin{align}
		\int \delta(a x) \varphi(x) dx = \int \frac{\delta(x)}{|a|} \varphi(x) dx, ~~\forall \varphi \in \text{Test Functions}
	\end{align} So now let's prove it!\\
	\\
	\emph{Proof:} Consider the LHS being integrated against an arbitrary test function. We'll find it becomes the RHS We'll have to do this twice, once with $a = |a| > 0$ and $a = -|a| < 0$. Let's do the positive case:
	\begin{align}
		\int_{-\infty}^\infty \delta (ax) \varphi (x) dx & = \int_{-\infty}^\infty \frac{\delta(x)}{a} \varphi(x/a) dx  & u\text{-sub}~ x \mapsto x/a\\
		& = \frac{1}{a} \varphi(0) & \text{By def: }\int \delta(x) g(x) dx = g(0)
	\end{align}
	So it effectively acts like $\delta(x)/a$. If you instead $u$-sub'ed $x\mapsto - x /|a|$, you'll have gotten $\varphi(0) / a$. Thus we've shown
	\begin{align}
		\int \delta(ax) \varphi(x) dx = \int \frac{\delta(x)}{|a|} \varphi(x) dx
	\end{align} QED.
\end{sidework}
	\item Symmetry: by consequence of scaling
		\begin{align}
			\delta(-x) = \delta(x)
		\end{align}
	\item Translation
		\begin{align}
			\int \varphi(x) \delta(x- x') = \varphi(x')
		\end{align} 
	\item Variational derivative:
	\begin{align}
		\frac{\delta}{\delta \varphi(x)} \varphi(y) = \delta(x-y)
	\end{align}
\end{itemize}



\begin{problem}[$\star$ Charge Distributions]
	The total charge $Q$ is related to the charge distribution $\rho(\mathbf r)$ by $Q = \int \rho(\mathbf r) d^3 r$. Write 
$\rho(\mathbf r)$ in terms of $\delta$ functions for the following systems:
	\begin{itemize}
		\item Point charge w/ total charge $Q$ centered at the origin. Do this in cartesian coordinates $(x,y,z)$, cylindrical coordinates $(\rho, \phi, z)$, and finally in spherical coordinates $(r, \phi, \theta)$.
		\item Infinite cylinder with radius $R$, with surface charge $\sigma$.
		\item Spherical shell of radius $R$, with total charge $Q$. 
	\end{itemize}
	\emph{Discussion:} How would you check your answer is correct? Recall equalities of expressions with delta functions are only defined in-terms of integrals, what is the meaning of what you've wrote down?
\end{problem}

\subsubsection{Showing something is a delta function in disguise}
In lecture, Grier mentioned the divergence of the vector field $\mathbf F(\mathbf r) = \frac{\mathbf{\hat{r}}}{\mathbf r^2}$ was
\begin{align}
	\boxed{\nabla \cdot \left ( \frac{\mathbf{\hat{r}}}{\mathbf r^2}\right) = 4\pi ~\delta^{(3)} (\mathbf r - \mathbf r')}
\end{align}
where $\nabla$ is the gradient operator.\\
\\
If you naively plug n' chug, you'd find $\nabla \cdot \left ( \frac{\mathbf{\hat{r}}}{\mathbf r^2}\right) =^? 0$. This is an artifact of a coordinate singularity (like how can you differentiate at $r \to 0$, it's not continuous). \\
\\
Grier makes a physics argument using $\nabla \cdot \mathbf E =  \rho / \epsilon_0$ as to why these are related. Here I'll show a mathematical proof.
\\
\\
\emph{Proof:} Recall a delta function is defined inside an integral, so we'll integrate this against an arbitrary scalar test function $\varphi : \mathbb R^3 \to \mathbb R$, and show it behaves like a delta function.\\
\\
First we can integrate by parts by noting $\text{div}(\vec f g) = (\nabla \cdot \vec f) g + \vec f \nabla g$
\begin{align}
	\int \varphi(\mathbf r) \nabla \cdot \left( \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3 r & = \underbrace{\int \nabla\cdot (\varphi(\mathbf r) \frac{\mathbf{\hat{r}}}{\mathbf r^2}) d^3r}_{\text{Term (1)}} - \underbrace{\int \nabla\varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} d^3r}_{\text{Term (2)}}
\end{align}


Term (1): Use the divergence theorem
\begin{align}
	\int_V \nabla\cdot \left(\varphi(\mathbf r) \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3r & = \int_{\partial V} \frac{\varphi(\mathbf r)}{\mathbf r^2}  \mathbf{\hat r} \cdot d\mathbf S \\
	& = 0 & \varphi \text{ vanishes at infinity}
\end{align}

Term (2): Use spherical coordinates
\begin{align}
	\int \nabla\varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} d^3r & = \int \nabla \varphi(\mathbf r) \cdot \frac{\mathbf{\hat{r}}}{\mathbf r^2} ~r^2 dr   d \Omega \\
	&= 4\pi   \int_0^\infty \frac{\partial \varphi}{\partial r}  dr \\
	& = 4\pi \varphi(r) \Big|_{r=0}^\infty & \text{Fundamental Theorem of Calculus}\\
	&  = 4\pi ( \cancelto{0}{\varphi(\infty)} - \varphi(0)) & \varphi \text{ vanishes at infinity}\\
	& = -4\pi \varphi(0)
\end{align}
Putting this all together
\begin{align}
	\int \varphi(\mathbf r) \nabla \cdot \left( \frac{\mathbf{\hat{r}}}{\mathbf r^2} \right) d^3 r = 4\pi \varphi(0)
\end{align}
Remember, if it acts like a delta function, it is a delta function by definition. QED.\\
\\
In general, proofs of this nature have lots of by parts calculations.
\begin{problem}[Representations of Delta Functions] The delta function emits an infinite number of representations. These are some common ones. Prove that they're actually delta functions!
	\begin{itemize}
		\item Laplacian Representation:
		\begin{align}
			\delta^{(3)} (\mathbf r - \mathbf r') = - \frac{1}{4\pi} \nabla^2 \frac{1}{|\mathbf r - \mathbf r'|}
		\end{align}
		where $|\cdot |$ is the magnitude of a vector.
		\item $(\star)$ Fourier Representation (very important for quantum mechanics!)
		\begin{align}
			\delta(x) = \int_{\mathbb R} \frac{d k}{2\pi} ~ e^{i k x}
		\end{align}
		
		\item Gaussian Representation
		\begin{align}
			\delta(x) = \lim_{\epsilon \to 0^+}  \frac{1}{\epsilon \sqrt{2\pi}} \exp \left(-\frac{1}{2} \frac{x^2}{\epsilon^2} \right)
		\end{align}
	\end{itemize}
	A good question to ask yourself, how do these generalize when $x \in \mathbb R^d$?
\end{problem}

\newpage
\section{Week 2: Gauss' Law}
This week we get some practice using Gauss' Law
\begin{problem}[$\star$ Spherical Gauss' Law]
	Consider a sphere of radius $R$ with total charge $Q$. The charge is spread out uniformly across the entire sphere. A common question is, what is the electric field $\mathbf E(\mathbf r)$ associated with this system? \\
	\\
	Let's derive this together using Gauss' Law. I'll ask you guiding questions to get you half the way there, and you piece together the rest!
	\begin{enumerate}
		\item Draw and write down the charge distribution $\rho(\mathbf r)$ of this system.
		\item How do we relate the electric field $\mathbf E(\mathbf r)$, the charge distribution $\rho(\mathbf r)$, and integration? 
		\item You've probably hit a point where you need to specify a test volume $\mathcal V$ to integrate over. What volume should you take? Why? Draw it 
		\item What is the surface area $S$ and volume $V$ of your chosen test volume $\mathcal V$?  
		\item Your integral equation in part (2) requires associated measures for said test surface $\mathcal S = \partial \mathcal V$ \& volume $\mathcal V$. So what are the corresponding measures of the surface integral $d S$ and volume integral $dV$? What vector do you associated to the measure of the surface integral $d \mathbf S$?
		\item What is the electric field $\mathbf E(\mathbf r)$ inside and outside the sphere. Show you recover a point charge outside.
	\end{enumerate}
	Note: on a test, you'll be asked to determine the electric field $\mathbf E(\mathbf r)$ without being told to use $\mathbf E(\mathbf r)$, and without any of the guiding in-between steps. Think about how you could have solve the problem end-to-end?
	\\
	\\
	Discussion:
	\begin{itemize}
		\item In part 2, why did I guide you towards integration? Why is the differential form of Gauss' Law not useful here?
		\item For this problem, which method did you prefer? Integrating the charge distribution $\mathbf E(\mathbf r) = \frac{1}{4\pi \epsilon_0} \int \frac{\rho(\mathbf r')}{\rcurs^2} \hrcurs d^3 r'$	or using the integral form of Gauss' Law $\int_{\partial V} \mathbf E(\mathbf r) \cdot d \mathbf S = \int_V \rho(\mathbf r) d^3 r $? Can you given an example system where you'd prefer using the other method?
		\item Notice you had symmetries in your charge distribution $\rho(\mathbf r)$. How does this information constrain the possible electric fields $\mathbf E(\mathbf r)$ that can result?
	\end{itemize}
\end{problem}
\begin{answer}
	Note: this isn't the answer to everything, but me just working through the problem for future reference. Not very pedagogical.\\
	\\
	To find the resulting electric field $\mathbf E(\mathbf r)$ (in spherical coordinates $\mathbf r = (r, \phi, \theta)$, first stat by writing down Gauss's Law in integral form
	\begin{align}
		\int_{\partial V} \mathbf E(\mathbf r) \cdot d \mathbf S = \frac{1}{\epsilon_0}\int \rho (\mathbf r) d^3 r
	\end{align}
	You always want the symmetry of your test volume $V$ to match the symmetry of the charge distribution. So you should choose $V$ to be a sphere of radius $r$.\\
	\\
	The charge distribution changes depending on whether you're inside or outside the sphere, so we should expect a change in electric field strength.\\
	\\
	Case: Inside sphere $(r \leq R)$.
	\begin{align}
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{1}{\epsilon_0} \frac{Q}{\frac{4}{3}\pi R^3} \int_{\Omega} \int_0^r  r^2 d\Omega\\
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{1}{4\pi \epsilon_0}\frac{3Q}{R^3} \left(\frac{r^3}{3}\right) \left(4\pi\right)\\
		\mathbf E(\mathbf r) \cdot \mathbf{\hat r} (r^2) (4\pi) & =  \frac{1}{4\pi \epsilon_0}\frac{3Q}{R^3} \left(\frac{r^3}{3}\right) \left(4\pi\right)\\
		\mathbf E(\mathbf r) & = \frac{1}{4\pi \epsilon_0} \frac{Q r}{R^3} \mathbf{\hat r}
	\end{align}
	where $d\Omega$ integrates over the solid-angle.\\
	\\
	Case: Outside sphere $(r > R)$
	\begin{align}
		\int_\Omega \mathbf E(\mathbf r) \cdot \mathbf{\hat r} ~ r^2 d\Omega & = \frac{Q_{encl}}{\epsilon_0} = \frac{Q}{\epsilon_0}\\
		\mathbf E(\mathbf r) & = \frac{1}{4\pi \epsilon_0}\frac{Q}{r^2} \mathbf{\hat r}
	\end{align}
	So your final answer is
	\begin{align}
		\boxed{\mathbf E(\mathbf r) = \begin{cases}
			\frac{1}{4\pi \epsilon_0} \frac{Q r}{R^3} \mathbf{\hat r} & r \leq R\\
			\frac{1}{4\pi \epsilon_0}\frac{Q}{r^2} \mathbf{\hat r} & r > R
		\end{cases}}
	\end{align}
	
\end{answer}

\begin{problem}[$\star$ Flux Conundrums]
	Here we explore a *slight* paradoxes which arise when we use Gauss' Law. 
	\begin{enumerate}
		\item Consider a uniformly charged sphere of radius $R$ with total charge $Q$. Use a test volume $\mathcal V$ which is a sphere of radius $r$. What is the electric flux inside and outside the sphere? What is the electric field (see previous problem)?
		\item Now add a point charge outside of the sphere at a distance $L$ away (that is $L > R$). Now using the same test volume $\mathcal V$, what is the electric flux inside and outside the sphere? What is the electric field now?
	\end{enumerate}
	What changed, what stayed the same. Why are you still a happy physicist?
\end{problem}



\newpage
\section{Week 3: Multivariable Calculus}
Ok Grier feels like the students had a couple questions about multivariable calculus, so I think it'd be good to take a recitation to do a basic review of some concepts

\subsection{Multivariable Integration by parts}
Integration by parts in multiple variables seems like a daunting. I mean there's so many new derivative operators, so how can you memorize them all? Well the secret is not to memorize, but to figure out how derive! \\
\\
Let's start w/ 1D integration by parts, and see how it generalizes to multiple dimensions.

\subsubsection{Integration by parts in 1D}
Recall the famous: integration by parts
\begin{align}
	\int_I u dv = u v \Big |_{\partial I} - \int_I v du
\end{align}
where $I$ is some interval on the real line (for example $I = [0, +\infty)$).\\
\\
\emph{Proof:}
Consider the total derivative of a function $f(x_1, ..., x_n)$
\begin{align}
	df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i
\end{align}
This has a product rule, meaning for functions $u$ and $v$:
\begin{align}
	d(uv) = (du) \cdot  v + u \cdot (dv)
\end{align}
\begin{problem}
	Prove the product rule of the total derivative.
\end{problem}
\begin{sidework}
	A small proof the product rule of the total derivative 
	\begin{align}
		d(fg) & =  \sum_i \frac{\partial(fg)}{\partial x_i}  dx_i \\
		& = \sum_i \left(\frac{\partial f}{\partial x_i} g + f \frac{\partial g}{\partial x_i}\right)dx_i \\
		& = \left(\sum_i \frac{\partial f}{\partial x_i} dx_i  \right)\cdot  g + f \cdot \left(\sum_i \frac{\partial g}{\partial x_i} dx_i  \right)\\
		& = (df)~g + f~ (dg)
	\end{align}
\end{sidework}
Notice the LHS of the integration by parts formula is $u (dv)$, so we can use our product rule from the total derivative to get
\begin{align}
	u dv = d(uv) - v du \implies \int u ~dv = \int d(uv) - \int v ~du
\end{align}
By the fundamental theorem of calculus $\int_I df = f \Big|_{\partial I}$. So you're left with
\begin{align}
	\int u~ dv = uv \Big|_{\partial I} - \int v ~du
\end{align}
QED.
\subsubsection{Integration in Multiple Variables}
The natural generalization of the derivative is the gradient $\nabla$. Now let's do the same derivation, and just turn the crank. \\
\\
For example, consider
\begin{align}
	\int_V f (\nabla g) ~ dV = \int_{\partial V} fg \mathbf ~d\mathbf S - \int_V g \nabla f ~ dV
\end{align}
\emph{Proof:} Notice the product rule
\begin{align}
	\nabla(fg) = (\nabla f) g + f (\nabla g) \implies f(\nabla g) = \nabla(fg) - (\nabla f) g
\end{align}
Do a volume integration over both sides
\begin{align}
	\int_V f(\nabla g) dV = \int_V \nabla (fg) dV - \int_V (\nabla f) g dV
\end{align}
To simplify the $\nabla(fg)$ term, we can use the divergence theorem. Consider the vector field $\mathbf F_i = fg ~ \mathbf{\hat e_i}$, where $\mathbf{\hat e_i}$ is the unit vector in the $i$'th direction. This means $\nabla(fg) = \sum_i \frac{\partial}{\partial x_i} \mathbf F_i = \sum_i (\nabla \cdot \mathbf F_i) \mathbf{\hat e_i}$.\begin{align}
	\int_V \nabla (fg) dV & = \sum_i \mathbf{\hat e_i} \int_V (\nabla \cdot \mathbf F_i)  dV \\
	& =  \sum_i \mathbf{\hat e_i}\int_{\partial V} fg ~ \mathbf{\hat e}_i\cdot d \mathbf S\\
	&  = \int_{\partial V} fg~ d\mathbf S
\end{align}
Putting it togther we find
\begin{align}
	\int_V f(\nabla g) dV = \int_{\partial V} fg ~d\mathbf S - \int_V g\nabla f~ dV
\end{align}
QED.
\begin{problem}
	Now it's your turn, prove the following multivariable integration by parts formulas
	\begin{align}
		\int_V \nabla f \cdot \nabla g ~ dV & = \int_{\partial V} f\nabla g \cdot d \mathbf S - \int_V f\nabla^2 g ~ dV\\
		\int_V (f \nabla^2 g - g \nabla^2 f) dV& = \int_{\partial V} (f \nabla g - g \nabla f) \cdot d \mathbf S
	\end{align}
	Think about what product rule do you need to get yourself there?
\end{problem}
\newpage

\section{Week 4: Harmonic Functions}
So in class, recall we derive the Laplace equation. We started at the differential form of Gauss' Law
\begin{align}
	\nabla \cdot \mathbf E = \rho / \epsilon_0
\end{align}
And then we said, there exists an electric potential (analogous to the potential $U$ in classical mechanics), defined as
\begin{align}
	\mathbf E = - \nabla V
\end{align}
Plugging this into your Gauss' law, and solving this s.t. the domain had no charge (that is $\rho = 0$). We found
\begin{align}
	\nabla^2 V = 0
\end{align}
This is \textbf{Laplace's equation}. This is a very special equation because (1) it's super general and appears in a bunch of other disciplines (i.e. fluid dynamics), and (2) it is difficult to solve (for particular boundary conditions) but we can make some very general statements about said equation.\\
\\
This equation is so important, we give its solutions a name:
\begin{definition}
	[Harmonic Functions] Consider a twice-differentiable function $V : \Omega \to \mathbb R$, where $\Omega$ is an open subset of $\mathbb R^n$. If this function satisfies Laplace's Equation
	\begin{align}
		\nabla^2 V = 0
	\end{align}
	everywhere on $\Omega$, then it is called a \textbf{harmonic function}.
\end{definition}

\subsection{Mean Value Property}
The most important fact about harmonic functions is that they always equal the average of their nearby values. This is known as the mean-value theorem.
\begin{theorem}
	Let $f: \mathbb R^3 \to \mathbb R$ be a harmonic function. Given a point $\mathbf p \in \mathbb R^3$, and $r > 0$, let $S_{\mathbf p}(r)$ be a sphere of radius $r$ at point $\mathbf p$. Then
	\begin{align}
		f(\mathbf p)&  = \frac{1}{4\pi r^2} \int_{S_{\mathbf p}(r)} f(x') dx'
	\end{align}
	In plain english. The value of the function at point $\mathbf p$ is equal to the average of said the function over it's neighbors.  
\end{theorem}
\emph{Proof:} Consider the function
\begin{align}
	A(r) = \frac{1}{4\pi r^2} \int_{S_{\mathbf p}(r)} f(x') dS'
\end{align}
where $dS$ is the integral over the surface area of the sphere $S_{\mathbf p}(r)$.
We'll show that $A(r)$ is a constant (w.r.t. $r$) and that it equals $f(\mathbf p)$\\
\\
Right now it's quite difficult to take the derivative of $r$ because it shows up in the bounds of the integral. What if we could make the bounds static, and place the $r$ dependence inside of the function?
\\
\\
The trick is to recall the definition of $S_{\mathbf p}(r)$
\begin{align}
	S_{\mathbf p}(r) = \{ \mathbf x \in \mathbb R^3 : (\mathbf x -\mathbf p)^2 = r^2\}
\end{align}
If we realize what this vector notation is say, it's saying that any points on the surface of the sphere $\mathbf x$, can be rewritten as the point $\mathbf p$ and a scaled unit-vector pointing radialy away from the center of the sphere $\mathbf{\hat n}$. This is exaclty
\begin{align}
	\mathbf{\hat n} = \frac{\mathbf x - \mathbf p}{|\mathbf x - \mathbf p|} = \frac{\mathbf x - \mathbf p}{r} \implies \mathbf x = \mathbf p + r \mathbf{\hat n}
\end{align}
So let's use this fact in attempting to remove the radial dependence from the 
\begin{align}
	A(r) & = \frac{1}{4\pi r^2} \int_{S_{\mathbf p}(r)} f(\mathbf x') dS'\\
	& = \frac{1}{4\pi r^2} \int_{S_{\mathbf p}(1)} f(\mathbf x') r^2 d\Omega' & \text{Wrote out surface integral}\\
	& = \frac{1}{4\pi} \int_{S_{\mathbf p}(1)} f(\mathbf p + r \mathbf{\hat n}') d\Omega'
\end{align}
Now we can take the derivative w.r.t. $r$
\begin{align}
	\frac{\partial A}{\partial r} & = \frac{1}{4\pi} \int_{S_p(1)} \frac{\partial}{\partial r}  f(\mathbf p + r \mathbf{\hat n}') d \Omega'\\
	& = \frac{1}{4\pi} \int_{S_p(1)} \nabla f(\mathbf p + r \mathbf{\hat n}')  \cdot \mathbf{\hat n}' d \Omega' & \text{Chain rule}\\
	& = \frac{1}{4\pi} \int_{S_p(1)} \nabla f(\mathbf x') \cdot \mathbf{\hat n}' d \Omega'& \text{Recall } \mathbf x' = \mathbf p + r \mathbf{\hat n}'\\
	& = \frac{1}{4\pi r^2} \int_{S_p(r)} \nabla f(\mathbf x') \cdot d \mathbf S' & \text{Change bounds of integral}\\
	& = \int_{B_p(r)} \nabla^2 f(\mathbf x') dV' & \text{Divergence theorem}\\
	& = 0 & \text{Assumed } f \text{ was harmonic} 
\end{align}
Now we've shown that $f(r) = C$ on the interval $r \in (0,R)$. Now we can just evaluate it any-point inside the interval to deduce the constant $C$. It's very convenient to consider the $r \to 0$, this is because the volume becomes infinitesimally small, evaluating $f$ only at the center $\mathbf p$. So basically
\begin{align}
	\lim_{r \to 0}A(r)  = 
	\lim_{r \to 0}\frac{1}{4\pi r^2} \int_{S_p(r)} f(\mathbf x') dS' =\int \delta(\mathbf x' -\mathbf p) f(\mathbf x') = f(\mathbf p)
\end{align}
If you can show the Gaussian with covariance going to zero is a delta function, you can basically do this proof. This is left as an exercise to the reader (lol I've always wanted to write that).\\
\\
Therefore
\begin{align}
	f(\mathbf p) = \frac{1}{4\pi r^2} \int_{S_{p}(r) } f(\mathbf x')dS'
\end{align}
QED.

\begin{problem}
 	There's a more elegant start of the Mean Value Property. It starts with Reynold's Transport Theorem
 	\begin{align}
 		\frac{d}{dr} \left(\int_{S(r)} u dS\right) = \int_{S_r} (\partial_n u + \frac{2}{r} u) dS 
 	\end{align}
 	Can you derive this, and then complete the proof?
\end{problem}

\subsection{Very Useful Corollaries}
\begin{corollary}[Uniqueness]
	If $f$ and $g$ are harmonic functions, and $f=g$ on the surface of a bounded region, then $f=g$ inside the region as well.
\end{corollary}
\begin{corollary}
	If $f$ is harmonic and has a local maximum or minimum, then $f$ is constant.
\end{corollary}
\begin{corollary}
	If $f$ is harmonic in a bounded solid region, then its absolute maximum and minimum occur on the boundary.
\end{corollary}


\subsection{What does this mean for physics?}
\begin{enumerate}
	\item Boundary value problems have unique solutions. So if you can find any solution which satisfies both laplace's equation and has the right boundary conditions, then you know you have the unique physical solution. For example, this is why method of images works.
	\item This tells us you can't trap charges using only static electric fields. All extrema of the potential occur on the boundary, not in the bulk (see Earnshaw's Theorem)
	\item ...
	
\end{enumerate}


\subsection{References:}
\begin{itemize}
	\item \url{https://web.math.ucsb.edu/~cmart07/Harmonic\%20functions.pdf}
\end{itemize}


\newpage
\section{Week 5: Series Expansions}
\begin{sidework}
	Grier is talking about separation of variables. He's talking about
	\begin{itemize}
		\item Fourier's theorem (how to expand things in a basis)
		\item Spherical harmonics \& expansions of special functions
		\item Strum Louiville Theory (behavior of eigenfunctions of Hermitian operators)
	\end{itemize}
	We should frame it as the laplacian operator is a Hermitian operator, and it's eigenvectors describe a complete orthogonal basis. Hence you can write a function as a superposition of said eigenvectors.
\end{sidework}
\begin{enumerate}
	\item Hermitian operators, so need to talk about self-adjointness
	\item Example problem: show the laplacian is a self-adjoint operator
	\item 
\end{enumerate}

If you're a STEM major, you have probably seen \href{https://www.youtube.com/watch?v=spUNpyF58BY}
{3Blue1Brown's video on Fourier Analysis} (if you haven't, you should watch it now!). Basically he visualizes how complex signals can be decomposed into a linear combinations of pure signals (sines and cosines). So if you haven't, go watch it now.

Now that you're back, I'll try to introduce it again, but lay the notation \& computational workflow for using this to solve some problems.

\subsection{Inner Products and Orthogonality}
Recall in linear algebra, we had these things called \textbf{inner products}. 
\begin{definition}
	[Inner Product] Let $V$ be a vector space. Consider the operation $\langle \cdot, \cdot \rangle : V \times V \to \mathbb R$. It is an \textbf{inner product} when it has the following properties:
	\\
	\\
	Let $a,b,c \in V$, and $\alpha \in \mathbb R$
	\begin{itemize}
		\item Bilineraity (linearity in both arguments): 
		\begin{align}
			\langle \alpha (a + b) , c\rangle & = \alpha \langle a, c\rangle  + \alpha \langle b , c\rangle\\
			\langle a, \alpha (b + c)\rangle &  = \alpha \langle a, b\rangle + \alpha \langle a, c\rangle
		\end{align}  
		\item Symmetry: $\langle a, b\rangle = \langle b, a\rangle$
		\item Positive-Definite: $\langle a, a\rangle \geq 0$, and equal if and only if $a=0$.
	\end{itemize}
\end{definition}
Ok cool definition, but this intuitively allowed us to determine how "aligned" two vectors were with each other. So for example, in $\mathbb R^3$, we had
\begin{align}
	\langle \vec a, \vec b\rangle = \vec a^T \vec b =  \sum_i a_i b_i =|a||b| \cos \theta
\end{align}
If $\langle \vec a , \vec b\rangle = 0$, then $\theta = 90 deg$ so they were \textbf{orthogonal} (perpendicular) to each other.\\
\\
What if we wanted to talk about more exotic objects, such as the inner product between functions? Well, imagine you index your function like a vector $f(x_i) = f_i$, then the dot product would look like $\sum_i f_i g_i = \sum_i f(x_i ) g(x_i)$, add a measure $\Delta x$, and you'd get
\begin{align}
	\langle f, g\rangle = \sum_i f(x_i) g(x_i) \Delta x \simeq \int f(x) g(x) dx
\end{align}
Now we can detect for "orthogonality" between two functions. 
\begin{problem}[$\star$ Orthogonality of Cosines and Sines]
	Show the following is a valid inner product:
	\begin{align}
		\langle f, g \rangle = \frac{1}{L} \int_{-L}^L f(x) g(x) dx
	\end{align}
	Show cosines and sines of integer frequency ($n,m \in \{1, 2, 3, ...\}$) are orthogonal:
	\begin{align}
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \sin\left(\frac{m \pi x}{L} \right) \right\rangle & = 0 \\
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \cos\left(\frac{m \pi x}{L} \right) \right\rangle & =   \delta_{nm} \\
		\left \langle \cos\left(\frac{n \pi x}{L}\right), \cos\left(\frac{m \pi x}{L} \right) \right\rangle  & =  \delta_{nm}
	\end{align}
	Online, you'll probably find different factors, just note the how the integral changes / the argument of the cosine \& sine.
\end{problem}

\subsection{Basis}
Recall in linear algebra that you had a basis to represent vectors. For example consider vectors $\mathbf v \in \mathbb R^3$, you can decompose it into a linear combination of basis vectors
\begin{align}
	\mathbf v = \begin{pmatrix}
		v_1 \\ v_2 \\ v_3 
	\end{pmatrix} = v_1 \begin{pmatrix}
		1 \\ 0 \\ 0
	\end{pmatrix} + v_2 \begin{pmatrix}
		0 \\ 1\\ 0
 	\end{pmatrix} + v_3 \begin{pmatrix}
 		0 \\ 0 \\ 1
 	\end{pmatrix}
\end{align}I'll denote the basis vectors as $\{\mathbf{\hat e}^{(i)}\}_{i=1}^3$. Lucky for us, I've chosen a set of orthogonal basis vectors, that is under an inner product $\langle \mathbf{\hat e}^{(i)}, \mathbf{\hat e}^{(j)} \rangle = \delta_{ij}$.


Sooo, what if your vector is in a really huge vector space $\mathbf v \in \mathbb R^d$? Well, same thing goes
\begin{align}
	\mathbf v = v_1 \begin{pmatrix}
		1 \\ 0 \\ 0 \\ \vdots
	\end{pmatrix} + v_2 \begin{pmatrix}
		0 \\ 1 \\ 0 \\ \vdots
	\end{pmatrix} + ... +  v_d \begin{pmatrix}
		\vdots \\ 0 \\ 0 \\ 1
	\end{pmatrix} = \sum_{i=1}^d v_i \mathbf{\hat e}^{(i)}
\end{align}
Ok, now keep this in your mind. Let's talk about functions, and I'll jump back to what I just talked about.\\
\\
So do you remember how you can Taylor series expand a function?
\begin{align}
	f(x) = \sum_{n=0}^\infty \underbrace{\frac{f^{(n)}(0)}{n!}}_{v_n} \underbrace{x^n}_{\mathbf{\hat e}^{(n)}}
\end{align}
Notice, this should remind you of the previous term. Heuristically, this look like your basis vectors are
\begin{align}
	f(x) = f(0) \begin{pmatrix}
		1 \\ 0 \\ 0 \\ \vdots
	\end{pmatrix} + f'(0) \begin{pmatrix}
		0 \\ x \\ 0 \\ \vdots
 	\end{pmatrix} + \frac{f''(0)}{2!} \begin{pmatrix}
 		0 \\ 0 \\ x^2 \\ \vdots
 	\end{pmatrix} + ...
\end{align}
Woah... So Clark what are you tryna say? Well, what if we could expand a function in an orthogonal basis? Whattt??

It takes a lot work, but basically the integer frequency cosines and sines form a complete basis. So now we can consider write a function a linear combination of the basis vectors (cosines and sines).
\begin{align}
	f(x) & = \sum_{n=0}^\infty A_n \cos\left(\frac{n \pi x}{L}\right) + \sum_{m=0}^\infty B_n \sin\left(\frac{n \pi x}{L}\right)\\
	& = A_0 + \sum_{n=1}^\infty A_n \cos\left(\frac{n \pi x}{L}\right) + \sum_{m=1}^\infty B_n \sin\left(\frac{n \pi x}{L}\right)
\end{align}

\subsection{Using Inner Products to Get Coefficients of Basis Vector}

Now you might ask: "Say I'm given an $f(x)$, how do I compute $A_0, A_n, B_n$?"
\\
\\
Let's do this in for discrete case, and then do it for functions. Consider the vector $v \in \mathbb R^d$ decomposed into its orthogonal basis
\begin{align} 
	\mathbf v = \sum_{i=1}^d v_i \mathbf{\hat e}^{(i)}
\end{align}
If I compute the inner product of this against an arbitrary basis vector, we find it picks out the associated coefficient
\begin{align}
	\mathbf{\hat e}^{(j) T}\mathbf v &= \sum_{i=1}^d v_i \mathbf{\hat e}^{(j) T} \mathbf{\hat e}^{(i)}\\
	\langle \mathbf{\hat e}^{(j)}, \mathbf v\rangle & = \sum_{i=1}^d v_i \langle \mathbf{\hat e}^{(j)},  \mathbf{\hat e}^{(i)} \rangle\\
	& =\sum_{i=1}^d v_i \delta_{ij}\\
	& = v_j
\end{align}
So let's use the same technique on derive the coefficients of the Fourier series. 
\begin{problem}[Coefficients of Fourier Series]
	Consider a function $f(x)$ represented in terms of its Fourier series
	\begin{align}
		f(x) = A_0 + \sum_{n=1}^\infty A_n \cos\left(\frac{n\pi x }{L}\right)  + \sum_{n=1}^\infty B_n \sin\left(\frac{n\pi x }{L}\right)
	\end{align}
	Derive a relationship between the coefficients and original function:
	\begin{align}
		A_0 & = \frac{1}{2L}\int_{-L}^L f(x) dx\\
		A_n & = \frac{1}{L}\int_{-L}^L f(x) \cos\left(\frac{n\pi x }{L}\right) dx\\
		B_n & = \frac{1}{L}\int_{-L}^L f(x) \sin\left(\frac{n\pi x }{L}\right) dx
	\end{align}
	Hint: Exploit the orthogonality that you've computed in the previous problem.
\end{problem}
Congrats! You've derived the Fourier series representation yourself! Be proud!

\subsection{Finding Basises}
A natural question to ask is, are there other expansions (other than cosines and sines)? And if so, how do we find them? This is where Spectral Theorem \& Strum Louiville Theory come in.
\\
\\
In quantum mechanics you learned about spectral theorem
\begin{theorem}
	[Spectral Theorem] If you have a Hermitian Operator $H$ (that is $H^\dagger = H$), then the eigenvectors form a complete orthonormal basis.
\end{theorem}
You've probably seen this in the context of finite-dimensional matrices, but you can extend this to infinite-dimensional matricies. The reason I bring up infinite dimensional matricies is you can think of matricies as maps from vector to vectors
\begin{align}
	M \vec v = \vec w \implies M(\vec v) = \vec w
\end{align}
This is exactly how derivatives work
\begin{align}
	\frac{d}{dx} (f) = g
\end{align}
So that means if you have a derivative operator which is "Hermitian" then its eigenvalues forms a complete orthonormal basis. Now, we have a way to find new basises.

The problem is that it is kinda non-sensical to complex transpose a derivative. This is where the notion of \textbf{self-adjoint} operators come in
\begin{definition}
	[Self-Adjoint] A operator $\mathcal L$ on some vector space with an inner product $\langle \cdot, \cdot \rangle$ is called self-adjoint if
	\begin{align}
		\langle f, \mathcal L g\rangle = \langle \mathcal L f , g\rangle
	\end{align}
	where $f,g$ are arbitrary vectors in the vector space.
\end{definition} 
\begin{problem}
	Show that a matrix is Hermitian if and only if it is self-adjoint w.r.t. the inner product
	\begin{align}
		\langle \psi, \phi\rangle = \psi^\dagger \phi
	\end{align}
	where $\dagger$ denotes the complex transpose.
\end{problem}
So we can see that Hermitian and Self-Adjointness are the same things as each other. So now one more question for you.
\begin{problem}
	Recall a slightly modified inner product we defined for cosines and sines
	\begin{align}
		\langle f, g \rangle = \int_a^b f^*(x) g(x) dx
	\end{align}
	Show the Sturm-Liouville Operator 
	\begin{align}
		\mathcal L[f] = \frac{d}{dx} \left[ p(x) \frac{df}{dx} \right] + q(x) f(x)
	\end{align}
	is self-adjoint w.r.t. this inner product. Note, this is self-adjoint with the boundary conditions:
	\begin{align}
		\left[p(x) \left(f \frac{dg}{dx} - g \frac{df}{dx}\right) \right]_{x=a}^b = 0
	\end{align}
\end{problem}
There are many ways to make this boundary condition go to zero. Here are the common ones
\begin{itemize}
	\item Dirichlet: $f(a) = f(b) = 0$
	\item Neumann: $p(a) \frac{df}{dx} \Big|_{x=a} = p(b) \frac{df}{dx} \Big|_{x=b} = 0$
	\item Periodic $f(a) = \pm f(b)$ and $\frac{df}{dx} \Big|_{x=a} = \pm p(b) \frac{df}{dx} \Big|_{x=b}$
\end{itemize}
You might find the Strum Liouville Operator vaguely familiar, it comes up in different coordinate representations in the Laplacian! That means we can find new basis by changing coordinates + applying various boundary conditions, and then solve the eigenvector problem that comes out.
\begin{problem}
	Consider the Laplacian in it's most simplest form: 1-dimension \& in cartesian coordinates.
	\begin{align}
		\nabla^2 = \frac{d^2}{dx^2}
	\end{align} 
	Find the eigenvector of this operator. These should look very very familiar, holy moly!
\end{problem}
So to recap
\begin{enumerate}
	\item The eigenvectors of a self-adjoint operators form a orthogonal basis.
	\item The Laplacian is self-adjoint, hence it's eigenvector $\nabla^2 f_n = \lambda_n f_n$ form a complete basis.
	\item Therefore we can Taylor expand any function by just considering the eigenvectors of the Laplacian.
\end{enumerate}
Wow!

\newpage
\section{Review for Midterm}
\subsection{Conceptual Stuff}
\subsubsection{Path Independence}
The electrostatic field is a conservative vector field.
\begin{align}
	\nabla \times \mathbf E = 0
\end{align}
This has two consequences
\begin{enumerate}
	\item Due to the curl free condition, you can determine if the field is physical (in the sense it is a solution to electrostatics) by just checking its curl.
	\item Consider computing the potential of an electric field. It is a line integral $\int_\gamma \mathbf E \cdot d\mathbf{l}$. Because the electric field is conservative, the line integral over it is independent of path, and is solely determined by the initial \& final position of the path.
\end{enumerate}

\subsubsection{Boundary Conditions}
Consider a problem where you had to find the electric field / potential where there was a boundary change. To be very specific (cause I personally think Griffith's notation is confusing).
\\
\\
Consider a surface $\partial \Omega$ with surface charge density $\sigma(\mathbf r)$ (this is for positions evaluated along the surface $\mathbf r \in \partial \Omega)$.  The electric field has the boundary condition
\begin{align}
	\mathbf E_{\text{outside}}(\mathbf r) - \mathbf E_{\text{inside}}(\mathbf r)  & = \frac{\sigma(\mathbf r)}{\epsilon_0} \mathbf{\hat n}(\mathbf r) & \forall \mathbf r \in \partial \Omega\\
	\implies \frac{\partial V_{\text{outside}}}{\partial \hat n} - \frac{\partial V_{\text{inside}}}{\partial \hat n} & = - \frac{\sigma(\mathbf r)}{\epsilon_0} & \forall \mathbf r \in \partial \Omega
\end{align}
where $\mathbf{\hat n(\mathbf r)}$ is the normal vector of the surface at point $\mathbf r$ along said surface. In plain English, the electric field is discontinuous across a surface, that jump is determined by the surface charge. Also the potential is continuous across the surface.
\begin{align}
	V_{\text{above}}(\mathbf r) &= V_{\text{below}} (\mathbf r) & \forall \mathbf r \in \partial \Omega
\end{align}

\subsection{Non-Exhaustive List of Equations You Should Know}

\begin{align}
    % Coulomb's Law
    \mathbf F &= \frac{1}{4\pi \varepsilon_0}\frac{qQ}{r^2} \hat{\mathbf r} \\[6pt]
    \mathbf F &= Q\mathbf E \\[12pt]
    % Electric Field
    \mathbf E(\mathbf r) &= \frac{1}{4\pi \varepsilon_0}\int \frac{\rho(\mathbf r')}{|\mathbf r - \mathbf r'|^2}\hat{\mathbf r} \, d\tau' \\[6pt]
    \oint \mathbf E \cdot d\mathbf a &= \frac{Q_{\text{enc}}}{\varepsilon_0} \\[6pt]
    \nabla \cdot \mathbf E &= \frac{\rho}{\varepsilon_0} \\[12pt]
    % Electric Potential
    V(\mathbf b) - V(\mathbf a) &= - \int_{\mathbf a}^{\mathbf b} \mathbf E \cdot d\mathbf l \\[6pt]
    V(\mathbf r) &= \frac{1}{4\pi \varepsilon_0} \int \frac{\rho(\mathbf r')}{|\mathbf r - \mathbf r'|} \, d\tau' \\[6pt]
    \mathbf E &= - \nabla V \\[12pt]
    % Boundary Conditions
    \varepsilon_{\text{above}}\mathbf E_{\text{above}} - \varepsilon_{\text{below}}\mathbf E_{\text{below}} &= \sigma_f \hat{\mathbf n} \\[6pt]
    \varepsilon_{\text{above}} \frac{\partial V_{\text{above}}}{\partial n} - \varepsilon_{\text{below}} \frac{\partial V_{\text{below}}}{\partial n} &= -\sigma_f \\[12pt]
    % Electric Potential Energy
    W &= \frac{1}{2}\sum_{i=1}^n q_i V(\mathbf r_i) \\[6pt]
    W &= \frac{\varepsilon_0}{2}\int_{\mathbb R^3} E^2 \, d\tau \\[12pt]
    % Poisson & Laplace Equation
    \nabla^2 V &= -\frac{\rho}{\varepsilon_0} \\[6pt]
    \nabla^2 V &= 0 \\[12pt]
    % Separable Spherical Solution
    V(r, \theta) &= \sum_{l=0}^\infty \left(A_l r^l + \frac{B_l}{r^{l+1}}\right) P_l(\cos\theta) \\[12pt]
    % Multipole Expansion
    V(\mathbf r) &= \frac{1}{4\pi \varepsilon_0} 
        \sum_{n=0}^\infty \frac{1}{r^{n+1}} 
        \int (r')^n P_n(\cos \alpha)\rho(\mathbf r')\, d\tau' \\[6pt]
    \mathbf p &= \int \mathbf r'\rho(\mathbf r')\, d\tau' \\[6pt]
    V_{\text{mono}}(\mathbf r) &= \frac{1}{4\pi \varepsilon_0}\frac{Q_{\text{total}}}{r} \\[6pt]
    V_{\text{dip}}(\mathbf r) &= \frac{1}{4\pi \varepsilon_0}\frac{\mathbf p \cdot \hat{\mathbf r}}{r^2}
\end{align}
\subsubsection{Also Math Identities}
Orthogonality of Cosines
\begin{align}
	\int_0^a \sin\left( \frac{n \pi x}{a} \right) \sin \left( \frac{m \pi x}{a} \right) dx & = \frac{a}{2} \delta_{nm}\\
	\int_0^a \cos\left( \frac{n \pi x}{a} \right) \cos \left( \frac{m \pi x}{a} \right) dx & = \frac{a}{2} \delta_{nm} \text{ (for $n,m \neq 0$)}
\end{align}
Orthogonality of Legendre Polynomials
\begin{align}
	\int_{-1}^1 P_{\ell}(x) P_{\ell'}(x) dx = \frac{2}{2 \ell + 1} \delta_{\ell \ell'}
\end{align}


\newpage
\subsection{Maxwell's Triangle}
\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.6]{assets/griffiths-triangle.png}
	\caption{The Griffith's Triangle, how to move from one quantity to another.}
\end{figure*}




\newpage
\section{Biot--Savart Law}
\subsection{Derivation}
Assume we're in the magnetostatic regime, where all currents are steady:
\begin{align}
    \nabla \times \mathbf{B} &= \mu_0 \mathbf{J} \\
    \nabla \cdot \mathbf{B} &= 0
\end{align}
Because $\nabla \cdot \mathbf{B}=0$, we may introduce the vector potential
$\mathbf{A}$ such that $\mathbf{B} = \nabla \times \mathbf{A}$. From here pick the Coulomb gauge $\nabla \cdot \mathbf{A} = 0$. You can now massage these to into your Maxwell's equations to get
\begin{align}
    \nabla \times \mathbf{B}
    &= \nabla \times (\nabla \times \mathbf{A}) \\
    &= \nabla(\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A} \\
    &= - \nabla^2 \mathbf{A}\\
    \implies - \nabla^2 \mathbf{A} & = \mu_0 \mathbf{J}
\end{align}
Note that each component of $\mathbf{A}$ satisfies a Poisson equation! 
To solve this equation, we can ansatz the solution as 
\begin{align}
	\mathbf{A}(\mathbf{r})
    = \frac{\mu_0}{4\pi}
      \int \frac{\mathbf{J}(\mathbf{r}')}{\|\brcurs\|}
      \, d^3 r'
\end{align}
This is because we have the delta function identity $\nabla^2 \!\left( \frac{1}{\|\brcurs\|} \right)
    = -4\pi\,\delta(\brcurs)$, where $\nabla$ acts on $\mathbf r$ only (not $\mathbf r'$). From here we can take the curl to get the resulting magnetic field
\begin{align}
    \mathbf{B}(\mathbf{r})
    &= \nabla \times \mathbf{A}(\mathbf{r}) \\
    &= \frac{\mu_0}{4\pi}
       \int
       \mathbf{J}(\mathbf{r}')
       \times
       \nabla \!\left( \frac{1}{\|\brcurs\|} \right)
       \, d^3 r'
\end{align}
\begin{align}
	\boxed{\mathbf{B}(\mathbf{r}) = \frac{\mu_0}{4\pi}
       \int
       \mathbf{J}(\mathbf{r}')
       \times
       \frac{\hat \brcurs}{\brcurs^2}
       \, d^3 r'}
\end{align}
\begin{sidework}
	\underline{Example:}  Let's recover the formula that Griffith's tell us. If you have a current density for a thin wire carrying current $I$, you have
\begin{align}
    \mathbf{J}(\mathbf{r}') \, d^3 r' = I \, d\boldsymbol{\ell}',
\end{align}
This means the fieldbecomes
\begin{align}
    \boxed{\mathbf{B}(\mathbf{r})
    = \frac{\mu_0}{4\pi}
      \, \int I \,
      \frac{d\boldsymbol{\ell}' \times \hat \brcurs}{\brcurs^2}}
\end{align}
\end{sidework}
\begin{problem}
	Consider a circular loop of radius $R$ and current $I$. Find the magnetic field along the centered $z$ axis.
\end{problem}
\begin{problem}
	Consider two parallel wires a distance $d$ apart and carry equal currents $I$ in the same direction (out of the page). Find the magnetic field at a point exactly midway between them.
\end{problem}


\newpage
\section{Farday's Law}
\begin{problem}[Induction Stove]
	An induction stove consists of coils below a ceramic surface. When driven by an AC current, the coil produces a time-varying magnetic field that threads the bottom of a conducting pan and induces eddy currents (which heats the pan).
	\\
	
	As a simplified model, consider the bottom of a pan as a thin conducting disk of radius $R$, thickness $\ell$, and electrical conductivity $\sigma$. The disk lies in the $xy$-plane. Assume that over the area of the disk the magnetic field is spatially uniform and points along $\hat z$
	\begin{align*}
		\mathbf B(t) = B_0 \cos (\omega\, t) \, \hat{\mathbf z}
	\end{align*}
	\begin{enumerate}
		\item Find the induced electric field $\mathbf E(\mathbf r, t)$ inside the disk.
		\item What is the current density $\mathbf J(\mathbf r, t)$ in the pan. Determine wether the current flows clockwise or counterclockwise (as when viewed above).
		\item By units analysis, we can give a heuristic for the "heating power" of our system
		\begin{align}
			P(t)= \int_{\Omega} \mathbf E(\mathbf r', t) \cdot \mathbf J(\mathbf r', t) d^3 \mathbf r'
		\end{align}
		where $\Omega$ is where the shape of the object with the induced current is (so it's the disk). What is this quantity? What is the time-averaged power $\langle P \rangle$ over one cycle?
		\item Why are induction stoves built the way that they are built?
		\begin{itemize}
			\item Why do they use high frequency AC, rather than the frequency of the wall $50/60$ Hz.
			\item Suppose you double the radius of the pan at the bottom? How does the heating power change?
			\item Why do glass / ceramic pans not heat up? 
		\end{itemize}
	\end{enumerate}
\end{problem}
\begin{answer}
	\begin{enumerate}
		\item Use Faraday's law
		\begin{align}
			\oint \mathbf E \cdot d \boldsymbol{\ell} = - \frac{d}{dt} \Phi_B
		\end{align}
		\begin{align}
			\text{LHS} & = \mathbf E\, (2\pi R) \\
			\text{RHS} & = - \frac{d}{dt} \left ( \pi R^2 B_0 \cos(\omega \, t) \right) =  \pi R^2 B_0 \omega \sin (\omega \, t)\\
			\implies \mathbf E(t)& = \frac{R}{2} B_0 \omega \, \sin(\omega \, t) \hat \phi
		\end{align}

		\item Use $\mathbf J = \sigma \mathbf E$
		\item Integrate your expression over one cycle (that is the period is $T = 2\pi/\omega$)
	\end{enumerate}
\end{answer}

\newpage
\section{Review of Waves}

Consider the wave equation
\begin{align}
	\nabla^2 \phi = \frac{1}{v^2} \frac{\partial^2 \phi}{\partial t^2}
\end{align}
where $\phi : \mathbb R^3 \times \mathbb R\to \mathbb R$ is a scalar field, $v$ is the speed of the propagating wave. Let's consider the 1d wave equation
\begin{align}
	\frac{\partial^2 \phi}{\partial x^2} = \frac{1}{v^2} \frac{\partial^2 \phi}{\partial t^2}
\end{align}
The generic solution is
\begin{align}
	\phi(x,t) = f(x - v\, t) + g(x + v\, t)
\end{align}
The reason this is a wave equation is that it says your intensity shifts in space, according to some time. Draw some diagrams and discuss.
\\
\\
Now let's derive this. Consider the change of coordinates 
\begin{align}
	r = x + v \, t,\ \  s = x - v\, t\\
	\implies x =  \frac{1}{2} (r + s) , \ t = \frac{1}{2v}(r - s)
\end{align}
So we ask what's the partial derivative w.r.t. the new coordinates? This involves chain rule
\begin{align}
	\frac{\partial }{\partial r} & =  \frac{\partial x}{\partial r} \frac{\partial }{\partial x}  + \frac{\partial t}{\partial r}\frac{\partial}{\partial t} = \frac{1}{2} \frac{\partial}{\partial x} + \frac{1}{2v}\frac{\partial }{\partial t} \right)\\
	\frac{\partial}{\partial s} & =  \frac{1}{2} \frac{\partial}{\partial x} - \frac{1}{2v}\frac{\partial }{\partial t}  \right)
\end{align}
This means the original wave equation gets a nice expression in the new coordinates

\begin{align}
	\frac{\partial^2 \phi}{\partial x^2} - \frac{1}{v^2} \frac{\partial^2 \phi}{\partial t^2}=  4 \frac{\partial^2 \phi}{\partial r \partial s} = 0
\end{align}
This has the solution
\begin{align}
	\phi = f(s) + g(r) = f(x -v \, t) + g(x+ v\, t)
\end{align}
QED.

\subsection{E\&M Waves in vacum}
Recall Maxwell's equations in the absence of charge \& current ($\rho = 0, \mathbf J = \mathbf 0$).
\begin{align}
	\nabla \cdot \mathbf E & = 0\\
	\nabla \cdot \mathbf B & = 0\\
	\nabla \times \mathbf E & = - \frac{\partial \mathbf B}{\partial t}\\
	\nabla \times \mathbf B & = \mu_0 \epsilon_0 \frac{\partial \mathbf E}{\partial t}
\end{align}
You can decouple the $\mathbf E$ and $\mathbf B$ fields by getting the 4th equation and applying the curl.
\begin{align}
	\nabla \times \nabla \times \mathbf B & = \mu_0 \epsilon_0 \frac{\partial }{\partial t} (\nabla \times \mathbf E )\\
	& = - \mu_0 \epsilon_0 \frac{\partial^2 \mathbf B}{\partial t^2} & \nabla \times \mathbf E
\end{align}
To simplify the RHS, you can use the vector identity
\begin{align}
	\nabla \times \nabla \times \mathbf B = \nabla(\nabla \cdot \mathbf B)- \nabla^2 \mathbf B
\end{align}
\begin{problem}
	Prove the previous identity
\end{problem}
Since $\mathbf B$ is divergence free ($\nabla \cdot \mathbf B = 0$), we get
\begin{align}
	\boxed{\nabla^2 \mathbf B = \mu_0 \epsilon_0 \frac{\partial^2 \mathbf B}{\partial t^2}}
\end{align}
So if we look at the components of $\mathbf B$, we have a wave equation where the speed of the wave propogates at $v = 1/\sqrt{\mu_0 \epsilon_0}$. Since we have experimentally measured $\mu_0, \epsilon_0$ you can plug this in and see what you get. What's amazing is that this is the speed of light!
\begin{problem}
	Redo the calculation but derive the wave equation on the $\mathbf E$ field
	\begin{align}
		\nabla^2 \mathbf E = \mu_0 \epsilon_0  \frac{\partial^2 \mathbf E}{\partial t^2}
	\end{align}
\end{problem}

\subsection{Lorentz Invariance}
One thing about the wave equation is that it naturally hints at Lorentz invariance (meaning it's an object that is acceptable according to special relativity). \\
\\
The key is that objects with the structure $a_\mu b^\mu$ are Lorentz invariant. That is if perform the map $\vec a \mapsto \Lambda \vec a, \vec b \mapsto \Lambda \vec b$, the object doesn't change under the mapping $a^\mu b_\mu \mapsto a^\mu b_\mu$. Physically, this means quantities $a_\mu b^\mu$ will have the same measurements according to all inertial observers.
\begin{problem}
	I want you to prove this for yourself. It might look daunting, but it's quite easy when you have all the definitions at hand. Recall...
\begin{align}
	a_\mu b^\mu \equiv \sum_{i,j} \eta_{ij} a^i b^j = \vec a^T \eta  \vec b
\end{align}
where $\eta$ is the metric
\begin{align}
\eta = \text{diag}(-1, +1, +1, +1)	
\end{align}
	and the Lorentz transformation goes as 
	\begin{align}
		\Lambda = \begin{pmatrix}
			\gamma & - \gamma \beta\\
			- \gamma \beta & \gamma
		\end{pmatrix}
	\end{align}
	where $v$ is speed of the frame of reference measured by "stationary observer", $\beta = v/c$, and $\gamma = 1/\sqrt{1 - \beta^2}$. 
\end{problem}


\begin{problem}
	Show the spacetime interval
	\begin{align}
		ds^2 = - dt^2 + dx^2
	\end{align}
	is Lorentz invariant. Hint, how can you rewrite $ds^2$ as an object of the following form $a^\mu a_\mu$?
\end{problem}
\begin{problem}
	Show the d'Almbertian 
	\begin{align}
		\Box \equiv c^2 \nabla^2 - \partial_t^2
	\end{align}
	is Lorentz invariant
\end{problem}



\newpage
\section{Review of Special Relativity}
Ok so last week I think I skipped ahead on relativity way too much. This is content from Hartle's book on General Relativity \& Tong's notes on special relativity.

\subsection{Lorentz Transformation}
Something that I'll ask you to remember
\begin{definition}
	[Lorentz Transformation]
	Consider two inertial frames $S, S'$, moving at a relative speed $v$. If observer $S$ sees the event at coordinates $(ct,x)$ and observer $S'$ sees the event at coordinates $(ct', x')$, these coordinates are related via
	\begin{align}
		ct' & = \gamma \left(ct- \beta \, x \right) \\
		x' & = \gamma\left( -\beta \, ct + x \right )\\
	\end{align} 
\end{definition}
Since we're grown-ups, we can look at this linear system of equations, and write it as a matrix multiplication
\begin{align}
	\begin{pmatrix}
		c t' \\ x'
	\end{pmatrix} & = \Lambda \begin{pmatrix}
		ct \\ x
	\end{pmatrix}\\
	\text{s.t. } \Lambda & = \begin{pmatrix}
		\gamma  & - \gamma \beta\\
		- \gamma \beta & \gamma
	\end{pmatrix}
\end{align}

\subsection{4-Position}
This motivates the first definition

\begin{definition}
	[4-Position]
	The 4-position are the spacetime coordinates of an event.
	\begin{align}
		X^\mu =  \begin{pmatrix}
			ct \\ \mathbf x
 		\end{pmatrix} = \begin{pmatrix}
 			ct \\ x \\ y \\ z
 		\end{pmatrix}
	\end{align}
	In this notation, the event is observed at time $t$, and at position $(x,y,z)$ by an inertial observer $S$.
\end{definition}
The invariant distance between the origin \& and a point in spacetime can be written as an inner product
\begin{align}
	\label{eqn:spacetime_interval_innerproduct}
	X \cdot X = \sum_{\mu} \eta_{\mu \mu} X^\mu X^\mu = X^T \eta X
\end{align}
The matrix $\eta$ is called the \textbf{metric}, in special relativity we use the \textbf{Minkowski metric}. This is found by solving Einstein's equations in flat spacetime (no objects with mass nor energy). It has the entries
\begin{align}
	\eta = \begin{pmatrix}
		-1 & 0 & 0 & 0\\
		0  & 1 & 0 & 0\\
		0  & 0 & 1 & 0\\
		0  & 0 & 0 & 1
	\end{pmatrix}
\end{align}
Using the expression for the Minkowski metric, we find (\ref{eqn:spacetime_interval_innerproduct}) becomes
\begin{align}
	X \cdot X = - c^2 t^2 + x^2 + y^2 + z^2 = - c^2 t^2 + \mathbf x \cdot \mathbf x
\end{align}
Notice this is the \textbf{spacetime interval} that you've derive in previous classes. 
\begin{align}
	\boxed{ds^2 = - c^2 dt^2 + dx^2}
\end{align}

\subsection{Lorentz Invariance}
An interesting property is the spacetime interval is Lorentz invariant, that is if you compare the spacetime interval of an event between two observers $S,S'$, moving at some relative speed, they'll say the spacetime interval will be the same. Traditionally, this is proved by plugging in the Lorentz transformation directly and finding it cancels. Let's prove it in a nicer way, using the matrix / vector notation
\begin{align}
	X' \cdot X' & = (X')^T \eta X'\\
	& = (\Lambda X)^T \eta (\Lambda X) & \text{Lorentz transform } X' = \Lambda X\\
	& = X^T (\Lambda^T \eta \Lambda) X\\
	& = X^T \eta X
\end{align}
QED.
\begin{problem}
	Prove the Minkowski metric has the following property $\Lambda^T \eta \Lambda = \eta$. This holds true for all speeds of the Lorentz transformation.
\end{problem}

This may seem new to you, but I promise you it's not. Consider the Euclidean inner product (that is your good old dot product from linear algebra)
\begin{align}
	\mathbf r \cdot \mathbf r = \mathbf r^T \mathbf r = x^2 + y^2 + z^2 = |r|^2
\end{align}
First, note the metric here is the identity matrix. Secondly, you can ask, what coordinate transformations operations leave the vector invariant. Since the dot product only depends on the vector's magnitude, you can do any rotation or flip and this will be invariant.

Mathematically, we can see this via a direct calculation. Let $\mathbf r' = M \mathbf r$ be the vector observed by a different observer
\begin{align}
	\mathbf r' \cdot \mathbf r' & = (\mathbf r')^T \mathbf r' = (M \mathbf r)^T  (M \mathbf r) = \mathbf r M^T M \mathbf r
\end{align} 
To get this to be invariant (that is $\mathbf r' \cdot \mathbf r' = \mathbf r \cdot \mathbf r$), you need $M^T M = \mathbb I$, meaning $M$ is an orthogonal matrix ($M^T = M^{-1}$), these are the rotations and flips!

\begin{problem}
	Consider two events with spacetime coordinates $X, \tilde X$ (observed by the same observer). Show $X \cdot \tilde X$ is Lorentz invariant.
\end{problem}
  
\subsection{Velocity addition}
By stacking two Lorentz transformations, we can derive the velocity addition formula.
\begin{align}
	\Lambda(v) =  \begin{pmatrix}
		\gamma & - \gamma \beta\\
		- \gamma \beta & \gamma
	\end{pmatrix}
\end{align}
In the notation, the Lorentz boost depends on velocity $v_1$, so the parenthesis are acting as a function argument. Notice that
\begin{align}
	\Lambda(v_1) \Lambda(v_2) = \begin{pmatrix}
		\gamma_1 & - \gamma_1 \beta_1\\
		- \gamma_1 \beta_1 & \gamma_1
	\end{pmatrix} \begin{pmatrix}
		\gamma_2 & - \gamma_2 \beta_2\\
		- \gamma_2 \beta_2 & \gamma_2 
	\end{pmatrix} = \Lambda\left( \frac{v_1 + v_2}{1 + \beta_1 \beta_2} \right)
\end{align}
In plain English, if you have two observers $S, S_2$, where $S_2$ is moving at a relative speed $v_2$ to observer $S$. An object moving at speed $v_1$ (measured by $S_2$), will be reported as having a speed
\begin{align}
	\boxed{v = \frac{v_1 + v_2}{1 + \beta_1 \beta_2}}
\end{align}
by observer $S$.

\subsection{Proper Time}




\begin{definition}
	[4-Momentum]
	The 4-momentum is defined as
	\begin{align}
		P^\mu = \begin{pmatrix}
			m c \gamma \\ m \gamma \mathbf u
		\end{pmatrix}
	\end{align}
	where $m$ is the rest mass of the particle, $\gamma = 1/\sqrt{1 - (v/c)^2}$, and $\mathbf u$ is the 3 velocity.
	
\end{definition}


\subsection{E\&M Revisited}
Newtons' Laws have a Lorentz invariant form
\begin{align}
	\frac{dP^\mu}{d\tau} = F^\mu
\end{align}
You rarely see this in classical mechanics because most forces are not valid in relativity. I.e. two blocks pushing against each other, the force is usually transmitted instantaneously, in SR, you have a limit to how fast things can travel (that is the speed of light).
\begin{align}
	F^\mu = \frac{q}{c} G^\mu_\nu U^\nu
\end{align}
where $U$ is the 4-velocity of a charged particle, and $G$ is the electromagnetic tensor.
\begin{align}
	G = \begin{pmatrix}
		0 & E_1 & E_2 & E_3\\
		E_1 & 0 & c B_3 & - cB_2 \\
		E_2 & -c B_3 & 0 & c B_1 \\
		E_3 & c B_2 & - cB_1 & 0
	\end{pmatrix}
\end{align}
(This tensor usually goes by the name $F^\mu_\nu$ but Tong renames it to prevent confusion from $F^\mu$ the force 4-vector). Where $E_i$ is the electric field in the $i$'th direction, and $B_i$ is the magnetic field in the $i$'th direction.
\begin{problem}
	Prove that spatial components give you the Lorentz force law that you know and love. Also show that time-component gives the work done $dW/dt = q \mathbf E \cdot \mathbf u$
\end{problem}


\newpage
\section{Appendix}

\subsection{Review of Vector Calculus}
\subsubsection{Coordinates}
In this section, and probably for everything else in these notes, we'll assume we work with functions that have domain on $\mathbb R^3$.\\
\\
Consider a vector $\vec r \in \mathbb R^3$. We can also decompose it a complete basis $\{\mathbf{\hat x}, \mathbf{\hat y}, \mathbf{\hat z}\}$, where the $\mathbf{\hat v}$ (hat symbol) signifies it is a unit vector.
\begin{align}
	\vec r  = x \mathbf{\hat x} + y \mathbf{\hat y} + z \mathbf {\hat z}
\end{align}





\subsubsection{Change of Coordinates of Differential Operators}
In classical mechanics, you probably had to work with differential operators. For example: the gradient $\vec \nabla$, the laplacian $\nabla^2 \equiv \vec \nabla \cdot \vec \nabla$, etc. However these have expressions depending on your coordinate system.
\begin{align}
	\vec \nabla & = \frac{\partial}{\partial x} \mathbf{\hat x} + \frac{\partial}{\partial y} \mathbf{\hat y} + \frac{\partial}{\partial z} \mathbf{\hat z}
\end{align}


 You might have noticed that certain problems are more amendable 












































\end{document}